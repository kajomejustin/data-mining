{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7f3ad9",
   "metadata": {},
   "source": [
    "# PART 1: SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea462a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 21:30:42,707 - INFO - Chromium init'ed with kwargs {}\n",
      "2025-07-13 21:30:42,714 - INFO - Found chromium path: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "2025-07-13 21:30:42,714 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpajeow_u1.\n",
      "2025-07-13 21:30:42,717 - INFO - Opening browser.\n",
      "2025-07-13 21:30:42,718 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpz_bkpxtc.\n",
      "2025-07-13 21:30:42,718 - INFO - Temporary directory at: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpz_bkpxtc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sentiment Analysis Results ===\n",
      "\n",
      "Total User Posts: 148\n",
      "User Posts Sentiment Distribution:\n",
      "Negative: 107 posts (72.3%)\n",
      "Neutral: 27 posts (18.2%)\n",
      "Positive: 14 posts (9.5%)\n",
      "\n",
      "Total @reg_rwanda Responses: 25\n",
      "@reg_rwanda Responses Sentiment Distribution:\n",
      "Neutral: 7 posts (28.0%)\n",
      "Negative: 9 posts (36.0%)\n",
      "Positive: 9 posts (36.0%)\n",
      "\n",
      "Common Locations Mentioned:\n",
      "- Rubavu: 16 mentions\n",
      "- Huye: 7 mentions\n",
      "- Kicukiro: 6 mentions\n",
      "- Kimironko: 6 mentions\n",
      "- Bugesera: 5 mentions\n",
      "- Nyarugenge: 5 mentions\n",
      "- Musanze: 5 mentions\n",
      "- Rwamagana: 3 mentions\n",
      "- Muyumbu: 3 mentions\n",
      "- Nyarukombe: 3 mentions\n",
      "- Nyamata: 2 mentions\n",
      "- Kirehe: 2 mentions\n",
      "- Kanombe: 2 mentions\n",
      "- Rubirizi: 2 mentions\n",
      "- Rubona: 2 mentions\n",
      "- Gasenga: 1 mentions\n",
      "- Ruhuha: 1 mentions\n",
      "- Nyundo: 1 mentions\n",
      "- Terimbere: 1 mentions\n",
      "- Keya: 1 mentions\n",
      "- Bumbogo: 1 mentions\n",
      "- Musave: 1 mentions\n",
      "- Rugando: 1 mentions\n",
      "- Nyamabuye: 1 mentions\n",
      "- Rusatira: 1 mentions\n",
      "- Kiruhura: 1 mentions\n",
      "- Kisimenti: 1 mentions\n",
      "- Nyamirama: 1 mentions\n",
      "- Kabutare: 1 mentions\n",
      "- Nyagahinga: 1 mentions\n",
      "- Rusororo: 1 mentions\n",
      "- Kagarama: 1 mentions\n",
      "- Kibungo: 1 mentions\n",
      "\n",
      "Common Issues Mentioned:\n",
      "- Umuriro: 62 mentions\n",
      "- Ikibazo: 25 mentions\n",
      "- Cashpower: 8 mentions\n",
      "- Outage: 4 mentions\n",
      "- Token: 3 mentions\n",
      "- Kizima: 3 mentions\n",
      "- Transformer: 2 mentions\n",
      "- Ubujura: 1 mentions\n",
      "- Low voltage: 1 mentions\n",
      "- Damaged: 1 mentions\n",
      "- Gucikagurika: 1 mentions\n",
      "- Meter: 1 mentions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 21:30:43,005 - INFO - Conforming 1 to file:///C:/Users/admin/AppData/Local/Temp/tmpajeow_u1/index.html\n",
      "2025-07-13 21:30:43,007 - INFO - Waiting on all navigates\n",
      "2025-07-13 21:30:44,629 - INFO - All navigates done, putting them all in queue.\n",
      "2025-07-13 21:30:44,632 - INFO - Getting tab from queue (has 1)\n",
      "2025-07-13 21:30:44,633 - INFO - Got 8083\n",
      "2025-07-13 21:30:44,634 - INFO - Processing User_Posts_Sentiment_Distribution.png\n",
      "2025-07-13 21:30:44,635 - INFO - Sending big command for User_Posts_Sentiment_Distribution.png.\n",
      "2025-07-13 21:30:44,717 - INFO - Sent big command for User_Posts_Sentiment_Distribution.png.\n",
      "2025-07-13 21:30:44,718 - INFO - Reloading tab 8083 before return.\n",
      "2025-07-13 21:30:44,812 - INFO - Putting tab 8083 back (queue size: 0).\n",
      "2025-07-13 21:30:44,813 - INFO - Waiting for all cleanups to finish.\n",
      "2025-07-13 21:30:44,813 - INFO - Exiting Kaleido\n",
      "2025-07-13 21:30:44,814 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:44,815 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:44,816 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:44,817 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:44,817 - INFO - Closing browser.\n",
      "2025-07-13 21:30:44,820 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:44,821 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:44,822 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:44,824 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:44,825 - INFO - Closing browser.\n",
      "2025-07-13 21:30:44,922 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:44,922 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:44,938 - INFO - Chromium init'ed with kwargs {}\n",
      "2025-07-13 21:30:44,954 - INFO - Found chromium path: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "2025-07-13 21:30:44,954 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpuu9xy1k6.\n",
      "2025-07-13 21:30:44,954 - INFO - Opening browser.\n",
      "2025-07-13 21:30:44,954 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp6bvdapv8.\n",
      "2025-07-13 21:30:44,954 - INFO - Temporary directory at: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp6bvdapv8\n",
      "2025-07-13 21:30:45,219 - INFO - Conforming 1 to file:///C:/Users/admin/AppData/Local/Temp/tmpuu9xy1k6/index.html\n",
      "2025-07-13 21:30:45,220 - INFO - Waiting on all navigates\n",
      "2025-07-13 21:30:46,845 - INFO - All navigates done, putting them all in queue.\n",
      "2025-07-13 21:30:46,849 - INFO - Getting tab from queue (has 1)\n",
      "2025-07-13 21:30:46,850 - INFO - Got 3D7F\n",
      "2025-07-13 21:30:46,851 - INFO - Processing REG_Rwanda_Response_Sentiment.png\n",
      "2025-07-13 21:30:46,852 - INFO - Sending big command for REG_Rwanda_Response_Sentiment.png.\n",
      "2025-07-13 21:30:46,931 - INFO - Sent big command for REG_Rwanda_Response_Sentiment.png.\n",
      "2025-07-13 21:30:46,933 - INFO - Reloading tab 3D7F before return.\n",
      "2025-07-13 21:30:47,070 - INFO - Putting tab 3D7F back (queue size: 0).\n",
      "2025-07-13 21:30:47,071 - INFO - Waiting for all cleanups to finish.\n",
      "2025-07-13 21:30:47,071 - INFO - Exiting Kaleido\n",
      "2025-07-13 21:30:47,072 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:47,074 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:47,075 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:47,076 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:47,077 - INFO - Closing browser.\n",
      "2025-07-13 21:30:47,083 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:47,085 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:47,086 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:47,087 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:47,088 - INFO - Closing browser.\n",
      "2025-07-13 21:30:47,295 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:47,295 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:47,309 - INFO - Chromium init'ed with kwargs {}\n",
      "2025-07-13 21:30:47,309 - INFO - Found chromium path: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "2025-07-13 21:30:47,309 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp_ccyvibf.\n",
      "2025-07-13 21:30:47,309 - INFO - Opening browser.\n",
      "2025-07-13 21:30:47,323 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpm1t364v1.\n",
      "2025-07-13 21:30:47,324 - INFO - Temporary directory at: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpm1t364v1\n",
      "2025-07-13 21:30:47,644 - INFO - Conforming 1 to file:///C:/Users/admin/AppData/Local/Temp/tmp_ccyvibf/index.html\n",
      "2025-07-13 21:30:47,646 - INFO - Waiting on all navigates\n",
      "2025-07-13 21:30:49,276 - INFO - All navigates done, putting them all in queue.\n",
      "2025-07-13 21:30:49,277 - INFO - Getting tab from queue (has 1)\n",
      "2025-07-13 21:30:49,278 - INFO - Got F0C5\n",
      "2025-07-13 21:30:49,278 - INFO - Processing Top_Locations_Mentioned.png\n",
      "2025-07-13 21:30:49,279 - INFO - Sending big command for Top_Locations_Mentioned.png.\n",
      "2025-07-13 21:30:49,386 - INFO - Sent big command for Top_Locations_Mentioned.png.\n",
      "2025-07-13 21:30:49,387 - INFO - Reloading tab F0C5 before return.\n",
      "2025-07-13 21:30:49,493 - INFO - Putting tab F0C5 back (queue size: 0).\n",
      "2025-07-13 21:30:49,494 - INFO - Waiting for all cleanups to finish.\n",
      "2025-07-13 21:30:49,494 - INFO - Exiting Kaleido\n",
      "2025-07-13 21:30:49,495 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:49,496 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:49,497 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:49,497 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:49,498 - INFO - Closing browser.\n",
      "2025-07-13 21:30:49,500 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:49,502 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:49,503 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:49,504 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:49,505 - INFO - Closing browser.\n",
      "2025-07-13 21:30:49,682 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:49,682 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:49,698 - INFO - Chromium init'ed with kwargs {}\n",
      "2025-07-13 21:30:49,698 - INFO - Found chromium path: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "2025-07-13 21:30:49,698 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmprp_w8cvd.\n",
      "2025-07-13 21:30:49,698 - INFO - Opening browser.\n",
      "2025-07-13 21:30:49,698 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp7q2hilej.\n",
      "2025-07-13 21:30:49,698 - INFO - Temporary directory at: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp7q2hilej\n",
      "2025-07-13 21:30:50,054 - INFO - Conforming 1 to file:///C:/Users/admin/AppData/Local/Temp/tmprp_w8cvd/index.html\n",
      "2025-07-13 21:30:50,056 - INFO - Waiting on all navigates\n",
      "2025-07-13 21:30:51,980 - INFO - All navigates done, putting them all in queue.\n",
      "2025-07-13 21:30:51,982 - INFO - Getting tab from queue (has 1)\n",
      "2025-07-13 21:30:51,983 - INFO - Got AF20\n",
      "2025-07-13 21:30:51,985 - INFO - Processing Top_Issues_Mentioned.png\n",
      "2025-07-13 21:30:51,986 - INFO - Sending big command for Top_Issues_Mentioned.png.\n",
      "2025-07-13 21:30:52,125 - INFO - Sent big command for Top_Issues_Mentioned.png.\n",
      "2025-07-13 21:30:52,126 - INFO - Reloading tab AF20 before return.\n",
      "2025-07-13 21:30:52,758 - INFO - Putting tab AF20 back (queue size: 0).\n",
      "2025-07-13 21:30:52,759 - INFO - Waiting for all cleanups to finish.\n",
      "2025-07-13 21:30:52,760 - INFO - Exiting Kaleido\n",
      "2025-07-13 21:30:52,762 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:52,764 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:52,764 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:52,765 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:52,766 - INFO - Closing browser.\n",
      "2025-07-13 21:30:52,768 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:52,770 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:52,771 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:52,772 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:52,773 - INFO - Closing browser.\n",
      "2025-07-13 21:30:53,024 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:53,024 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:53,044 - INFO - Chromium init'ed with kwargs {}\n",
      "2025-07-13 21:30:53,059 - INFO - Found chromium path: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "2025-07-13 21:30:53,059 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpo7wwem5q.\n",
      "2025-07-13 21:30:53,059 - INFO - Opening browser.\n",
      "2025-07-13 21:30:53,075 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp_8cfym76.\n",
      "2025-07-13 21:30:53,075 - INFO - Temporary directory at: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp_8cfym76\n",
      "2025-07-13 21:30:53,401 - INFO - Conforming 1 to file:///C:/Users/admin/AppData/Local/Temp/tmpo7wwem5q/index.html\n",
      "2025-07-13 21:30:53,403 - INFO - Waiting on all navigates\n",
      "2025-07-13 21:30:54,912 - INFO - All navigates done, putting them all in queue.\n",
      "2025-07-13 21:30:54,914 - INFO - Getting tab from queue (has 1)\n",
      "2025-07-13 21:30:54,915 - INFO - Got 722A\n",
      "2025-07-13 21:30:54,917 - INFO - Processing Sentiment_Distribution_Comparison_User_Posts_vs_REG_Responses.png\n",
      "2025-07-13 21:30:54,918 - INFO - Sending big command for Sentiment_Distribution_Comparison_User_Posts_vs_REG_Responses.png.\n",
      "2025-07-13 21:30:55,026 - INFO - Sent big command for Sentiment_Distribution_Comparison_User_Posts_vs_REG_Responses.png.\n",
      "2025-07-13 21:30:55,029 - INFO - Reloading tab 722A before return.\n",
      "2025-07-13 21:30:55,145 - INFO - Putting tab 722A back (queue size: 0).\n",
      "2025-07-13 21:30:55,147 - INFO - Waiting for all cleanups to finish.\n",
      "2025-07-13 21:30:55,149 - INFO - Exiting Kaleido\n",
      "2025-07-13 21:30:55,153 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:55,157 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:55,160 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:55,162 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:55,164 - INFO - Closing browser.\n",
      "2025-07-13 21:30:55,169 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:55,171 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:55,174 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:55,176 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:55,180 - INFO - Closing browser.\n",
      "2025-07-13 21:30:55,491 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:55,497 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:55,576 - INFO - Chromium init'ed with kwargs {}\n",
      "2025-07-13 21:30:55,592 - INFO - Found chromium path: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "2025-07-13 21:30:55,592 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpi2c7u4kh.\n",
      "2025-07-13 21:30:55,592 - INFO - Opening browser.\n",
      "2025-07-13 21:30:55,607 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp8wk1ejy3.\n",
      "2025-07-13 21:30:55,639 - INFO - Temporary directory at: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp8wk1ejy3\n",
      "2025-07-13 21:30:55,909 - INFO - Conforming 1 to file:///C:/Users/admin/AppData/Local/Temp/tmpi2c7u4kh/index.html\n",
      "2025-07-13 21:30:55,911 - INFO - Waiting on all navigates\n",
      "2025-07-13 21:30:57,371 - INFO - All navigates done, putting them all in queue.\n",
      "2025-07-13 21:30:57,373 - INFO - Getting tab from queue (has 1)\n",
      "2025-07-13 21:30:57,375 - INFO - Got 3901\n",
      "2025-07-13 21:30:57,376 - INFO - Processing Sentiment_Distribution_by_Location.png\n",
      "2025-07-13 21:30:57,377 - INFO - Sending big command for Sentiment_Distribution_by_Location.png.\n",
      "2025-07-13 21:30:57,519 - INFO - Sent big command for Sentiment_Distribution_by_Location.png.\n",
      "2025-07-13 21:30:57,523 - INFO - Reloading tab 3901 before return.\n",
      "2025-07-13 21:30:57,629 - INFO - Putting tab 3901 back (queue size: 0).\n",
      "2025-07-13 21:30:57,630 - INFO - Waiting for all cleanups to finish.\n",
      "2025-07-13 21:30:57,631 - INFO - Exiting Kaleido\n",
      "2025-07-13 21:30:57,639 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:57,642 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:57,645 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:57,647 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:57,649 - INFO - Closing browser.\n",
      "2025-07-13 21:30:57,658 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:57,665 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:57,669 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:30:57,673 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:30:57,677 - INFO - Closing browser.\n",
      "2025-07-13 21:30:58,009 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:58,010 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:58,025 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:58,027 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:58,028 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:58,030 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:58,031 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:58,033 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:58,035 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:58,036 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:58,037 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:58,040 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:30:58,041 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:30:58,043 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:31:00,391 - INFO - Chromium init'ed with kwargs {}\n",
      "2025-07-13 21:31:00,424 - INFO - Found chromium path: C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\n",
      "2025-07-13 21:31:00,427 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmpjbn9l2o_.\n",
      "2025-07-13 21:31:00,428 - INFO - Opening browser.\n",
      "2025-07-13 21:31:00,428 - INFO - Temp directory created: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp16ykg7w1.\n",
      "2025-07-13 21:31:00,459 - INFO - Temporary directory at: C:\\Users\\admin\\AppData\\Local\\Temp\\tmp16ykg7w1\n",
      "2025-07-13 21:31:00,723 - INFO - Conforming 1 to file:///C:/Users/admin/AppData/Local/Temp/tmpjbn9l2o_/index.html\n",
      "2025-07-13 21:31:00,724 - INFO - Waiting on all navigates\n",
      "2025-07-13 21:31:03,271 - INFO - All navigates done, putting them all in queue.\n",
      "2025-07-13 21:31:03,273 - INFO - Getting tab from queue (has 1)\n",
      "2025-07-13 21:31:03,273 - INFO - Got 903E\n",
      "2025-07-13 21:31:03,274 - INFO - Processing REG_Rwanda_Response_Effectiveness_Metrics.png\n",
      "2025-07-13 21:31:03,275 - INFO - Sending big command for REG_Rwanda_Response_Effectiveness_Metrics.png.\n",
      "2025-07-13 21:31:03,351 - INFO - Sent big command for REG_Rwanda_Response_Effectiveness_Metrics.png.\n",
      "2025-07-13 21:31:03,353 - INFO - Reloading tab 903E before return.\n",
      "2025-07-13 21:31:03,449 - INFO - Putting tab 903E back (queue size: 0).\n",
      "2025-07-13 21:31:03,449 - INFO - Waiting for all cleanups to finish.\n",
      "2025-07-13 21:31:03,450 - INFO - Exiting Kaleido\n",
      "2025-07-13 21:31:03,452 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:31:03,453 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:31:03,454 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:31:03,454 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:31:03,455 - INFO - Closing browser.\n",
      "2025-07-13 21:31:03,457 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:31:03,459 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:31:03,460 - INFO - Cancelling tasks.\n",
      "2025-07-13 21:31:03,460 - INFO - Exiting Kaleido/Choreo\n",
      "2025-07-13 21:31:03,461 - INFO - Closing browser.\n",
      "2025-07-13 21:31:03,724 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:31:03,725 - INFO - shutil.rmtree worked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Visualization Summary ===\n",
      "✓ User Sentiment Distribution (user_sentiment_distribution.png)\n",
      "✓ REG Response Sentiment Distribution (reg_sentiment_distribution.png)\n",
      "✓ Top Locations Mentioned (top_locations.png)\n",
      "✓ Top Issues Mentioned (top_issues.png)\n",
      "✓ Sentiment Comparison Chart (sentiment_comparison.png)\n",
      "✓ Geographic Sentiment Analysis (sentiment_by_location.png)\n",
      "✓ Negative Sentiment Word Cloud (negative_wordcloud.png)\n",
      "✓ Response Effectiveness Metrics (response_effectiveness.png)\n",
      "\n",
      "All visualizations have been saved as individual PNG files!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the CSV dataset\n",
    "try:\n",
    "    df = pd.read_csv('x.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'x.csv' not found. Please provide the correct file path.\")\n",
    "    exit()\n",
    "\n",
    "# Keyword lists\n",
    "positive_keywords = [\n",
    "    'murakoze', 'thank you', 'mwakoze', 'kudufasha', 'power is back', 'resolved', 'gushima',\n",
    "    'fixed', 'restored', 'solved', 'byakozwe', 'amashanyarazi yagarutse'\n",
    "]\n",
    "negative_keywords = [\n",
    "    'wabuze', 'outage', 'problem', 'issue', 'ikibazo', 'ibibazo', 'mbi',\n",
    "    'bad service', 'no power', 'low voltage', 'kizima', 'gucikagurika', 'delay',\n",
    "    'faulty', 'not working', 'frustration', 'ubujura', 'power cut', 'turaheba',\n",
    "    'biranze', 'birakomeye', 'byagoranye', 'muke', 'byanze', 'ntibikunda',\n",
    "    'ntibikora', 'biragoye', 'guhagarara', 'byatewe niki', 'isaha ishize',\n",
    "    'hashize umwanya', 'hari ibura', 'ntacyo bigeze', 'no electricity',\n",
    "    'without power', 'several hours', 'ntiyakunze', 'gufite ikibazo'\n",
    "]\n",
    "neutral_keywords = [\n",
    "    'ese', 'ryari', 'when', 'where', 'how', 'update', 'question', 'menya', 'kuki',\n",
    "    'a handi', 'byagenze gute', 'habaye iki', 'bizakemuka', 'ni gute', 'a ho',\n",
    "    'twegereje', 'turagutegereza', 'kindly assist', 'mwaramutse', 'mwiriwe',\n",
    "    'turacyategereje', 'please assist', 'what happened', 'panne', 'electricity',\n",
    "    'cashpower', 'token', 'damaged'\n",
    "]\n",
    "\n",
    "# Location and issue lists\n",
    "locations = [\n",
    "    'bugesera', 'kirehe', 'kicukiro', 'huye', 'rwamagana', 'kanombe', 'kimironko',\n",
    "    'musanze', 'nyarugenge', 'rubavu', 'nyamirama', 'rusatira', 'kiruhura', 'gikondo',\n",
    "    'kabutare', 'nyagahinga', 'rusororo', 'kagarama', 'kibungo', 'kayonza', 'karongi',\n",
    "    'rubengera', 'muyumbu', 'nyamabuye', 'rwanfro', 'bumbogo', 'musave', 'rugando',\n",
    "    'nyakabanda', 'nyacyonga', 'kisimenti', 'rubirizi', 'nyundo', 'terimbere', 'keya',\n",
    "    'gahara', 'murehe', 'dagaza', 'nyamata', 'gasenga', 'ruhuha', 'nyarukombe', 'rubona'\n",
    "]\n",
    "issues = [\n",
    "    'outage', 'low voltage', 'cashpower', 'token', 'umuriro', 'ikibazo', 'ibibazo',\n",
    "    'power cut', 'transformer', 'technical issue', 'not working', 'damaged', 'delay',\n",
    "    'kizima', 'gucikagurika', 'faulty', 'ubujura', 'line', 'meter', 'panne'\n",
    "]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return ''\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return text.lower()\n",
    "\n",
    "def fuzzy_match(keyword, text):\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if fuzz.ratio(keyword, word) > 75:\n",
    "            return True, keyword\n",
    "    if keyword in text:\n",
    "        return True, keyword\n",
    "    return False, None\n",
    "\n",
    "def classify_sentiment(text, is_reg_response=False):\n",
    "    text = preprocess_text(text)\n",
    "    matched_keywords = []\n",
    "    if is_reg_response:\n",
    "        for kw in positive_keywords:\n",
    "            matched, matched_kw = fuzzy_match(kw, text)\n",
    "            if matched and not any(fuzzy_match(nkw, text)[0] for nkw in negative_keywords):\n",
    "                matched_keywords.append(f\"positive: {matched_kw}\")\n",
    "                return 'positive', matched_keywords\n",
    "        for kw in negative_keywords:\n",
    "            matched, matched_kw = fuzzy_match(kw, text)\n",
    "            if matched and all(k not in text for k in ['murakoze', 'mwiriwe', 'kugira ngo', 'tukabikurikirana']):\n",
    "                matched_keywords.append(f\"negative: {matched_kw}\")\n",
    "                return 'negative', matched_keywords\n",
    "        matched_keywords.append('neutral: reg_response')\n",
    "        return 'neutral', matched_keywords\n",
    "\n",
    "    for kw in negative_keywords:\n",
    "        matched, matched_kw = fuzzy_match(kw, text)\n",
    "        if matched:\n",
    "            matched_keywords.append(f\"negative: {matched_kw}\")\n",
    "            if 'cyane' in text and any(fuzzy_match(nkw, text)[0] for nkw in ['mbi', 'ntibikunda', 'biragoye']):\n",
    "                matched_keywords.append('negative: mbi cyane')\n",
    "            return 'negative', matched_keywords\n",
    "\n",
    "    for kw in positive_keywords:\n",
    "        matched, matched_kw = fuzzy_match(kw, text)\n",
    "        if matched and not any(fuzzy_match(nkw, text)[0] for nkw in negative_keywords):\n",
    "            matched_keywords.append(f\"positive: {matched_kw}\")\n",
    "            return 'positive', matched_keywords\n",
    "\n",
    "    for kw in neutral_keywords:\n",
    "        matched, matched_kw = fuzzy_match(kw, text)\n",
    "        if matched:\n",
    "            matched_keywords.append(f\"neutral: {matched_kw}\")\n",
    "            return 'neutral', matched_keywords\n",
    "\n",
    "    for loc in locations:\n",
    "        matched, matched_loc = fuzzy_match(loc, text)\n",
    "        if matched:\n",
    "            matched_keywords.append(f\"neutral: {matched_loc} (location-only)\")\n",
    "            return 'neutral', matched_keywords\n",
    "\n",
    "    matched_keywords.append('neutral: no match')\n",
    "    return 'neutral', matched_keywords\n",
    "\n",
    "# Combine text columns\n",
    "df['raw_text'] = df['css-1jxf684 5'].fillna('') + ' ' + df['css-1jxf684 4'].fillna('')\n",
    "df['combined_text'] = df['raw_text'].apply(preprocess_text)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['css-1jxf684 2', 'raw_text', 'css-146c3p1 href'])\n",
    "\n",
    "# Filter out irrelevant rows\n",
    "irrelevant_phrases = [\n",
    "    'show more replies', 'who to follow', 'click to follow', 'official twitter',\n",
    "    'ministry of', 'rwandair', 'national carrier', 'minisiteri', r'\\d{2}/\\d{2}/\\d{4}',\n",
    "    r'\\[\\d{4}-\\d{4}-\\d{4}-\\d{4}-\\d{4}\\]', 'konteri', 'abdallah eneya',\n",
    "    r'^\\d+\\s', r'^\\s*@[\\w\\s]+$'\n",
    "]\n",
    "df = df[~df['raw_text'].str.contains('|'.join(irrelevant_phrases), case=False, na=False)]\n",
    "df['word_count'] = df['raw_text'].apply(lambda x: len(str(x).split()))\n",
    "df = df[df['word_count'] >= 7]\n",
    "df['has_content'] = df['combined_text'].apply(\n",
    "    lambda x: any(fuzzy_match(kw, x)[0] for kw in negative_keywords + positive_keywords + neutral_keywords + locations)\n",
    ")\n",
    "df = df[df['has_content']]\n",
    "\n",
    "# Separate user posts and @reg_rwanda responses\n",
    "user_posts = df[df['css-1jxf684 2'] != '@reg_rwanda'].copy()\n",
    "reg_posts = df[df['css-1jxf684 2'] == '@reg_rwanda'].copy()\n",
    "\n",
    "# Apply sentiment classification\n",
    "user_posts['sentiment_result'] = user_posts['combined_text'].apply(lambda x: classify_sentiment(x, is_reg_response=False))\n",
    "user_posts['sentiment'] = user_posts['sentiment_result'].apply(lambda x: x[0])\n",
    "reg_posts['sentiment_result'] = reg_posts['combined_text'].apply(lambda x: classify_sentiment(x, is_reg_response=True))\n",
    "reg_posts['sentiment'] = reg_posts['sentiment_result'].apply(lambda x: x[0])\n",
    "\n",
    "# Count sentiment distribution\n",
    "user_sentiment_counts = Counter(user_posts['sentiment'])\n",
    "total_user_posts = sum(user_sentiment_counts.values())\n",
    "user_sentiment_percentages = {\n",
    "    sentiment: (count / total_user_posts * 100) if total_user_posts > 0 else 0\n",
    "    for sentiment, count in user_sentiment_counts.items()\n",
    "}\n",
    "reg_sentiment_counts = Counter(reg_posts['sentiment'])\n",
    "total_reg_posts = sum(reg_sentiment_counts.values())\n",
    "reg_sentiment_percentages = {\n",
    "    sentiment: (count / total_reg_posts * 100) if total_reg_posts > 0 else 0\n",
    "    for sentiment, count in reg_sentiment_counts.items()\n",
    "}\n",
    "\n",
    "# Extract locations and issues\n",
    "location_counts = Counter()\n",
    "issue_counts = Counter()\n",
    "for _, row in user_posts.iterrows():\n",
    "    text = row['combined_text']\n",
    "    for loc in locations:\n",
    "        if re.search(r'\\b' + re.escape(loc) + r'\\b', text, re.IGNORECASE):\n",
    "            location_counts[loc] += 1\n",
    "    for issue in issues:\n",
    "        if re.search(r'\\b' + re.escape(issue) + r'\\b', text, re.IGNORECASE):\n",
    "            issue_counts[issue] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"=== Sentiment Analysis Results ===\")\n",
    "print(f\"\\nTotal User Posts: {total_user_posts}\")\n",
    "print(\"User Posts Sentiment Distribution:\")\n",
    "for sentiment, count in user_sentiment_counts.items():\n",
    "    print(f\"{sentiment.capitalize()}: {count} posts ({user_sentiment_percentages[sentiment]:.1f}%)\")\n",
    "print(f\"\\nTotal @reg_rwanda Responses: {total_reg_posts}\")\n",
    "print(\"@reg_rwanda Responses Sentiment Distribution:\")\n",
    "for sentiment, count in reg_sentiment_counts.items():\n",
    "    print(f\"{sentiment.capitalize()}: {count} posts ({reg_sentiment_percentages[sentiment]:.1f}%)\")\n",
    "print(\"\\nCommon Locations Mentioned:\")\n",
    "for location, count in location_counts.most_common():\n",
    "    print(f\"- {location.capitalize()}: {count} mentions\")\n",
    "print(\"\\nCommon Issues Mentioned:\")\n",
    "for issue, count in issue_counts.most_common():\n",
    "    print(f\"- {issue.capitalize()}: {count} mentions\")\n",
    "\n",
    "# ============== VISUALIZATIONS ==============\n",
    "\n",
    "# 1. User Sentiment Distribution (Individual Pie Chart)\n",
    "fig1 = go.Figure()\n",
    "user_sentiments = list(user_sentiment_counts.keys())\n",
    "user_values = list(user_sentiment_counts.values())\n",
    "colors_user = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "fig1.add_trace(\n",
    "    go.Pie(labels=user_sentiments, values=user_values, \n",
    "           marker_colors=colors_user, name=\"User Posts\",\n",
    "           textinfo='label+percent', textposition='inside')\n",
    ")\n",
    "\n",
    "fig1.update_layout(\n",
    "    title='User Posts Sentiment Distribution',\n",
    "    title_x=0.5,\n",
    "    height=400,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "# Save as PNG\n",
    "fig1.write_image('results/user_sentiment_distribution.png', width=800, height=400, scale=2)\n",
    "\n",
    "# 2. REG Response Sentiment Distribution (Individual Pie Chart)\n",
    "fig2 = go.Figure()\n",
    "reg_sentiments = list(reg_sentiment_counts.keys())\n",
    "reg_values = list(reg_sentiment_counts.values())\n",
    "colors_reg = ['#96CEB4', '#FECA57', '#FF9FF3']\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Pie(labels=reg_sentiments, values=reg_values,\n",
    "           marker_colors=colors_reg, name=\"REG Responses\",\n",
    "           textinfo='label+percent', textposition='inside')\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='REG Rwanda Response Sentiment',\n",
    "    title_x=0.5,\n",
    "    height=400,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "# Save as PNG\n",
    "fig2.write_image('results/reg_sentiment_distribution.png', width=800, height=400, scale=2)\n",
    "\n",
    "# 3. Top Locations Mentioned (Individual Bar Chart)\n",
    "fig3 = go.Figure()\n",
    "top_locations = location_counts.most_common(10)\n",
    "if top_locations:\n",
    "    loc_names = [loc[0].capitalize() for loc in top_locations]\n",
    "    loc_counts = [loc[1] for loc in top_locations]\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Bar(x=loc_names, y=loc_counts, name=\"Locations\",\n",
    "               marker_color='#6C5CE7', text=loc_counts, textposition='auto')\n",
    "    )\n",
    "\n",
    "fig3.update_layout(\n",
    "    title='Top Locations Mentioned',\n",
    "    title_x=0.5,\n",
    "    xaxis_title='Location',\n",
    "    yaxis_title='Number of Mentions',\n",
    "    height=400,\n",
    "    font=dict(size=12),\n",
    "    xaxis_tickangle=45\n",
    ")\n",
    "\n",
    "# Save as PNG\n",
    "fig3.write_image('results/top_locations.png', width=800, height=400, scale=2)\n",
    "\n",
    "# 4. Top Issues Mentioned (Individual Bar Chart)\n",
    "fig4 = go.Figure()\n",
    "top_issues = issue_counts.most_common(10)\n",
    "if top_issues:\n",
    "    issue_names = [issue[0].capitalize() for issue in top_issues]\n",
    "    issue_counts_list = [issue[1] for issue in top_issues]\n",
    "    \n",
    "    fig4.add_trace(\n",
    "        go.Bar(x=issue_names, y=issue_counts_list, name=\"Issues\",\n",
    "               marker_color='#FD79A8', text=issue_counts_list, textposition='auto')\n",
    "    )\n",
    "\n",
    "fig4.update_layout(\n",
    "    title='Top Issues Mentioned',\n",
    "    title_x=0.5,\n",
    "    xaxis_title='Issue',\n",
    "    yaxis_title='Number of Mentions',\n",
    "    height=400,\n",
    "    font=dict(size=12),\n",
    "    xaxis_tickangle=45\n",
    ")\n",
    "\n",
    "# Save as PNG\n",
    "fig4.write_image('results/top_issues.png', width=800, height=400, scale=2)\n",
    "\n",
    "# 5. Sentiment Comparison Chart\n",
    "fig5 = go.Figure()\n",
    "\n",
    "sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "user_percentages = [user_sentiment_percentages.get(s.lower(), 0) for s in sentiments]\n",
    "reg_percentages = [reg_sentiment_percentages.get(s.lower(), 0) for s in sentiments]\n",
    "\n",
    "fig5.add_trace(go.Bar(\n",
    "    name='User Posts',\n",
    "    x=sentiments,\n",
    "    y=user_percentages,\n",
    "    marker_color='#FF6B6B',\n",
    "    text=[f'{p:.1f}%' for p in user_percentages],\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig5.add_trace(go.Bar(\n",
    "    name='REG Responses',\n",
    "    x=sentiments,\n",
    "    y=reg_percentages,\n",
    "    marker_color='#4ECDC4',\n",
    "    text=[f'{p:.1f}%' for p in reg_percentages],\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig5.update_layout(\n",
    "    title='Sentiment Distribution Comparison: User Posts vs REG Responses',\n",
    "    title_x=0.5,\n",
    "    xaxis_title='Sentiment',\n",
    "    yaxis_title='Percentage (%)',\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "# Save as PNG\n",
    "fig5.write_image('results/sentiment_comparison.png', width=1000, height=500, scale=2)\n",
    "\n",
    "# 6. Geographic Sentiment Analysis\n",
    "if location_counts:\n",
    "    # Create a more detailed location analysis\n",
    "    location_sentiment = {}\n",
    "    for loc in locations:\n",
    "        loc_posts = user_posts[user_posts['combined_text'].str.contains(loc, case=False, na=False)]\n",
    "        if len(loc_posts) > 0:\n",
    "            loc_sentiment_counts = Counter(loc_posts['sentiment'])\n",
    "            location_sentiment[loc] = {\n",
    "                'negative': loc_sentiment_counts.get('negative', 0),\n",
    "                'positive': loc_sentiment_counts.get('positive', 0),\n",
    "                'neutral': loc_sentiment_counts.get('neutral', 0),\n",
    "                'total': len(loc_posts)\n",
    "            }\n",
    "    \n",
    "    if location_sentiment:\n",
    "        loc_df = pd.DataFrame(location_sentiment).T\n",
    "        loc_df = loc_df.sort_values('total', ascending=False).head(15)\n",
    "        \n",
    "        fig6 = go.Figure()\n",
    "        \n",
    "        fig6.add_trace(go.Bar(\n",
    "            name='Negative',\n",
    "            x=loc_df.index,\n",
    "            y=loc_df['negative'],\n",
    "            marker_color='#FF6B6B'\n",
    "        ))\n",
    "        \n",
    "        fig6.add_trace(go.Bar(\n",
    "            name='Neutral',\n",
    "            x=loc_df.index,\n",
    "            y=loc_df['neutral'],\n",
    "            marker_color='#45B7D1'\n",
    "        ))\n",
    "        \n",
    "        fig6.add_trace(go.Bar(\n",
    "            name='Positive',\n",
    "            x=loc_df.index,\n",
    "            y=loc_df['positive'],\n",
    "            marker_color='#4ECDC4'\n",
    "        ))\n",
    "        \n",
    "        fig6.update_layout(\n",
    "            title='Sentiment Distribution by Location',\n",
    "            title_x=0.5,\n",
    "            xaxis_title='Location',\n",
    "            yaxis_title='Number of Posts',\n",
    "            barmode='stack',\n",
    "            height=600,\n",
    "            xaxis={'categoryorder': 'total descending'}\n",
    "        )\n",
    "        \n",
    "        fig6.update_xaxes(tickangle=45)\n",
    "        \n",
    "        # Save as PNG\n",
    "        fig6.write_image('results/sentiment_by_location.png', width=1200, height=600, scale=2)\n",
    "\n",
    "# 7. Word Cloud for Negative Sentiments (Unchanged, already static)\n",
    "negative_posts = user_posts[user_posts['sentiment'] == 'negative']\n",
    "if len(negative_posts) > 0:\n",
    "    negative_text = ' '.join(negative_posts['combined_text'].tolist())\n",
    "    \n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white',\n",
    "                         colormap='Reds',\n",
    "                         max_words=100).generate(negative_text)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud: Negative Sentiment Posts', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/negative_wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# 8. Response Effectiveness Analysis\n",
    "if len(reg_posts) > 0:\n",
    "    response_effectiveness = {\n",
    "        'Total User Posts': len(user_posts),\n",
    "        'Total REG Responses': len(reg_posts),\n",
    "        'Response Rate': (len(reg_posts) / len(user_posts) * 100) if len(user_posts) > 0 else 0,\n",
    "        'Positive Responses': reg_sentiment_counts.get('positive', 0),\n",
    "        'Negative User Posts': user_sentiment_counts.get('negative', 0),\n",
    "        'Resolution Rate': (reg_sentiment_counts.get('positive', 0) / user_sentiment_counts.get('negative', 1) * 100)\n",
    "    }\n",
    "    \n",
    "    metrics = ['Response Rate (%)', 'Resolution Rate (%)']\n",
    "    values = [response_effectiveness['Response Rate'], response_effectiveness['Resolution Rate']]\n",
    "    \n",
    "    fig7 = go.Figure(go.Bar(\n",
    "        x=metrics,\n",
    "        y=values,\n",
    "        text=[f'{v:.1f}%' for v in values],\n",
    "        textposition='auto',\n",
    "        marker_color=['#4ECDC4', '#96CEB4']\n",
    "    ))\n",
    "    \n",
    "    fig7.update_layout(\n",
    "        title='REG Rwanda Response Effectiveness Metrics',\n",
    "        title_x=0.5,\n",
    "        yaxis_title='Percentage (%)',\n",
    "        height=400,\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "    \n",
    "    # Save as PNG\n",
    "    fig7.write_image('results/response_effectiveness.png', width=800, height=400, scale=2)\n",
    "\n",
    "# 9. Sentiment Timeline (if timestamp data available)\n",
    "# This would require timestamp data - placeholder for future enhancement\n",
    "print(\"\\n=== Visualization Summary ===\")\n",
    "print(\"✓ User Sentiment Distribution (user_sentiment_distribution.png)\")\n",
    "print(\"✓ REG Response Sentiment Distribution (reg_sentiment_distribution.png)\")\n",
    "print(\"✓ Top Locations Mentioned (top_locations.png)\")\n",
    "print(\"✓ Top Issues Mentioned (top_issues.png)\")\n",
    "print(\"✓ Sentiment Comparison Chart (sentiment_comparison.png)\")\n",
    "print(\"✓ Geographic Sentiment Analysis (sentiment_by_location.png)\")\n",
    "print(\"✓ Negative Sentiment Word Cloud (negative_wordcloud.png)\")\n",
    "print(\"✓ Response Effectiveness Metrics (response_effectiveness.png)\")\n",
    "print(\"\\nAll visualizations have been saved as individual PNG files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c21c9e",
   "metadata": {},
   "source": [
    "# PART 2: WEB SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e37b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 21:31:03,737 - INFO - TemporaryDirectory.cleanup() worked.\n",
      "2025-07-13 21:31:03,738 - INFO - shutil.rmtree worked.\n",
      "2025-07-13 21:31:03,755 - INFO - Starting to scrape: https://citations.ouest-france.fr/citations-thomas-gatabazi-18498.html\n",
      "2025-07-13 21:31:03,756 - INFO - Attempt 1, Strategy 1: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWeb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 2: WEB SCRAPING FROM REAL WEBSITE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 21:31:07,454 - WARNING - ✗ Strategy 1 failed with status: 403\n",
      "2025-07-13 21:31:07,456 - INFO - Attempt 1, Strategy 2: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWeb...\n",
      "2025-07-13 21:31:10,507 - WARNING - ✗ Strategy 2 failed with status: 403\n",
      "2025-07-13 21:31:10,515 - INFO - Attempt 1, Strategy 3: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWeb...\n",
      "2025-07-13 21:31:13,207 - INFO - ✓ Success with Strategy 3\n",
      "2025-07-13 21:31:13,253 - INFO - Page title: Citations célèbres de Thomas Gatabazi, 260+ pensées inspirantes\n",
      "2025-07-13 21:31:13,255 - INFO - Page length: 118048 characters\n",
      "2025-07-13 21:31:13,266 - INFO - Found 40 elements with selector: blockquote\n",
      "2025-07-13 21:31:13,329 - INFO - Successfully extracted 40 quotes\n",
      "2025-07-13 21:31:13,329 - INFO - Data saved to results/quotes.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "WEB SCRAPING REPORT\n",
      "============================================================\n",
      "Total quotes scraped: 40\n",
      "Unique authors: 1\n",
      "Average quote length: 90.8 characters\n",
      "Median quote length: 80.5 characters\n",
      "Shortest quote: 40 characters\n",
      "Longest quote: 174 characters\n",
      "\n",
      "Selectors used:\n",
      "  - blockquote: 40 quotes\n",
      "\n",
      "Sample quotes:\n",
      "  1. \"Les leçons de la vie se donnent tous les jours mais les moments difficiles permettent de mieux les c...\" - Thomas Gatabazi\n",
      "  2. \"Les difficultés de la vie, après les avoir surmontées, deviennent les lampes qui éclairent notre che...\" - Thomas Gatabazi\n",
      "  3. \"Les pires moments de la vie enseignent les meilleures leçons de la vie....\" - Thomas Gatabazi\n",
      "\n",
      "Files generated:\n",
      "  - quotes.csv: CSV format\n",
      "\n",
      "✓ Web scraping completed successfully!\n",
      "✓ 40 quotes extracted and saved\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class QuoteScraper:    \n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.scraped_data = []\n",
    "        \n",
    "    def get_headers_strategies(self):\n",
    "        \"\"\"Return different header strategies to bypass anti-bot measures\"\"\"\n",
    "        return [\n",
    "            # Strategy 1: Chrome Windows\n",
    "            {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                'Accept-Language': 'en-US,en;q=0.9,fr;q=0.8',\n",
    "                'Accept-Encoding': 'gzip, deflate, br',\n",
    "                'Connection': 'keep-alive',\n",
    "                'Upgrade-Insecure-Requests': '1',\n",
    "                'Sec-Fetch-Dest': 'document',\n",
    "                'Sec-Fetch-Mode': 'navigate',\n",
    "                'Sec-Fetch-Site': 'none'\n",
    "            },\n",
    "            \n",
    "            # Strategy 4: With Google Referer\n",
    "            {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "                'Referer': 'https://www.google.com/',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                'Accept-Language': 'en-US,en;q=0.9'\n",
    "            },\n",
    "            \n",
    "            # Strategy 5: Minimal headers\n",
    "            {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def make_request(self, url, max_retries=3):\n",
    "        \"\"\"Make request with multiple strategies and retry logic\"\"\"\n",
    "        \n",
    "        strategies = self.get_headers_strategies()\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            for i, headers in enumerate(strategies):\n",
    "                try:\n",
    "                    logger.info(f\"Attempt {attempt + 1}, Strategy {i + 1}: {headers.get('User-Agent', 'Unknown')[:50]}...\")\n",
    "                    \n",
    "                    # Random delay to avoid rate limiting\n",
    "                    time.sleep(random.uniform(1, 3))\n",
    "                    \n",
    "                    response = self.session.get(\n",
    "                        url, \n",
    "                        headers=headers, \n",
    "                        timeout=15,\n",
    "                        allow_redirects=True\n",
    "                    )\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        logger.info(f\"✓ Success with Strategy {i + 1}\")\n",
    "                        return response\n",
    "                    else:\n",
    "                        logger.warning(f\"✗ Strategy {i + 1} failed with status: {response.status_code}\")\n",
    "                        \n",
    "                except requests.RequestException as e:\n",
    "                    logger.error(f\"✗ Strategy {i + 1} failed with error: {e}\")\n",
    "                    continue\n",
    "                \n",
    "            # Wait before retry\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = random.uniform(5, 10)\n",
    "                logger.info(f\"Waiting {wait_time:.1f} seconds before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_quotes(self, soup, url):\n",
    "        \"\"\"Extract quotes using multiple selectors and strategies\"\"\"\n",
    "        \n",
    "        quotes_data = []\n",
    "        \n",
    "        # Multiple selector strategies for different quote website structures\n",
    "        quote_selectors = [\n",
    "            # Common quote selectors\n",
    "            'div.quote',\n",
    "            'div.citation',\n",
    "            'blockquote',\n",
    "            'div[class*=\"quote\"]',\n",
    "            'div[class*=\"citation\"]',\n",
    "            'p.quote',\n",
    "            '.quote-text',\n",
    "            '.citation-text',\n",
    "            'div.quotation',\n",
    "            'span.quote',\n",
    "            # Specific to French quote sites\n",
    "            'div.citation-contenu',\n",
    "            'div.citation-texte',\n",
    "            'p.citation',\n",
    "            'div[class*=\"texte\"]'\n",
    "        ]\n",
    "        \n",
    "        author_selectors = [\n",
    "            '.author',\n",
    "            '.citation-auteur',\n",
    "            'div.auteur',\n",
    "            'span.author',\n",
    "            'div[class*=\"author\"]',\n",
    "            'div[class*=\"auteur\"]',\n",
    "            'cite',\n",
    "            'footer'\n",
    "        ]\n",
    "        \n",
    "        # Try each selector strategy\n",
    "        for selector in quote_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            if elements:\n",
    "                logger.info(f\"Found {len(elements)} elements with selector: {selector}\")\n",
    "                \n",
    "                for element in elements:\n",
    "                    quote_text = element.get_text(strip=True)\n",
    "                    \n",
    "                    # Filter out very short or very long texts\n",
    "                    if quote_text and 10 <= len(quote_text) <= 1000:\n",
    "                        \n",
    "                        # Try to find author\n",
    "                        author = self.extract_author(element, author_selectors)\n",
    "                        if not author:\n",
    "                            author = \"Thomas Gatabazi\"  # Default as per original requirement\n",
    "                        \n",
    "                        quotes_data.append({\n",
    "                            'quote': quote_text,\n",
    "                            'author': author,\n",
    "                            'source': url,\n",
    "                            'selector_used': selector,\n",
    "                            'quote_length': len(quote_text),\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                \n",
    "                if quotes_data:\n",
    "                    break  # Use first successful selector\n",
    "        \n",
    "        # Fallback: Extract from paragraphs if no quotes found\n",
    "        if not quotes_data:\n",
    "            logger.info(\"No quotes found with standard selectors. Trying paragraph extraction...\")\n",
    "            quotes_data = self.extract_from_paragraphs(soup, url)\n",
    "        \n",
    "        return quotes_data\n",
    "    \n",
    "    def extract_author(self, element, author_selectors):\n",
    "        \"\"\"Extract author information from quote element\"\"\"\n",
    "        \n",
    "        # Look for author in parent or sibling elements\n",
    "        for selector in author_selectors:\n",
    "            # Check within the element\n",
    "            author_elem = element.select_one(selector)\n",
    "            if author_elem:\n",
    "                return author_elem.get_text(strip=True)\n",
    "            \n",
    "            # Check in parent element\n",
    "            parent = element.parent\n",
    "            if parent:\n",
    "                author_elem = parent.select_one(selector)\n",
    "                if author_elem:\n",
    "                    return author_elem.get_text(strip=True)\n",
    "            \n",
    "            # Check in next sibling\n",
    "            next_sibling = element.find_next_sibling()\n",
    "            if next_sibling:\n",
    "                author_elem = next_sibling.select_one(selector)\n",
    "                if author_elem:\n",
    "                    return author_elem.get_text(strip=True)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_from_paragraphs(self, soup, url):\n",
    "        \"\"\"Fallback method: extract potential quotes from paragraphs\"\"\"\n",
    "        \n",
    "        quotes_data = []\n",
    "        paragraphs = soup.find_all('p')\n",
    "        \n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            \n",
    "            # Heuristics for identifying quotes\n",
    "            if (text and \n",
    "                20 <= len(text) <= 500 and\n",
    "                not text.lower().startswith(('copyright', 'tous droits', 'mentions légales')) and\n",
    "                not any(word in text.lower() for word in ['cookie', 'privacy', 'politique'])):\n",
    "                \n",
    "                quotes_data.append({\n",
    "                    'quote': text,\n",
    "                    'author': 'Thomas Gatabazi',\n",
    "                    'source': url,\n",
    "                    'selector_used': 'paragraph_fallback',\n",
    "                    'quote_length': len(text),\n",
    "                    'scraped_at': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        return quotes_data\n",
    "    \n",
    "    def scrape_quotes(self, url):\n",
    "        \"\"\"Main scraping method\"\"\"\n",
    "        \n",
    "        logger.info(f\"Starting to scrape: {url}\")\n",
    "        \n",
    "        # Make request\n",
    "        response = self.make_request(url)\n",
    "        \n",
    "        if not response:\n",
    "            logger.error(\"All request strategies failed\")\n",
    "            return []\n",
    "        \n",
    "        # Parse HTML\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Debug information\n",
    "            title = soup.title.string if soup.title else 'No title'\n",
    "            logger.info(f\"Page title: {title}\")\n",
    "            logger.info(f\"Page length: {len(response.text)} characters\")\n",
    "            \n",
    "            # Extract quotes\n",
    "            quotes_data = self.extract_quotes(soup, url)\n",
    "            \n",
    "            if quotes_data:\n",
    "                logger.info(f\"Successfully extracted {len(quotes_data)} quotes\")\n",
    "                self.scraped_data.extend(quotes_data)\n",
    "            else:\n",
    "                logger.warning(\"No quotes found on this page\")\n",
    "            \n",
    "            return quotes_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing response: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_to_csv(self, filename='results/quotes.csv'):\n",
    "        \"\"\"Save scraped data to CSV file\"\"\"\n",
    "        \n",
    "        if not self.scraped_data:\n",
    "            logger.warning(\"No data to save\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            df = pd.DataFrame(self.scraped_data)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            logger.info(f\"Data saved to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving to CSV: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a comprehensive report of the scraping results\"\"\"\n",
    "        \n",
    "        if not self.scraped_data:\n",
    "            logger.warning(\"No data to generate report\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.scraped_data)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"WEB SCRAPING REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Total quotes scraped: {len(df)}\")\n",
    "        print(f\"Unique authors: {df['author'].nunique()}\")\n",
    "        print(f\"Average quote length: {df['quote_length'].mean():.1f} characters\")\n",
    "        print(f\"Median quote length: {df['quote_length'].median():.1f} characters\")\n",
    "        print(f\"Shortest quote: {df['quote_length'].min()} characters\")\n",
    "        print(f\"Longest quote: {df['quote_length'].max()} characters\")\n",
    "        \n",
    "        print(f\"\\nSelectors used:\")\n",
    "        selector_counts = df['selector_used'].value_counts()\n",
    "        for selector, count in selector_counts.items():\n",
    "            print(f\"  - {selector}: {count} quotes\")\n",
    "        \n",
    "        print(f\"\\nSample quotes:\")\n",
    "        for i, row in df.head(3).iterrows():\n",
    "            print(f\"  {i+1}. \\\"{row['quote'][:100]}...\\\" - {row['author']}\")\n",
    "        \n",
    "        print(f\"\\nFiles generated:\")\n",
    "        print(f\"  - quotes.csv: CSV format\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"PART 2: WEB SCRAPING FROM REAL WEBSITE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize scraper\n",
    "    url = 'https://citations.ouest-france.fr/citations-thomas-gatabazi-18498.html'\n",
    "    scraper = QuoteScraper(url)\n",
    "    \n",
    "    # Scrape quotes\n",
    "    quotes = scraper.scrape_quotes(url)\n",
    "    \n",
    "    if quotes:\n",
    "        # Save data in multiple formats\n",
    "        scraper.save_to_csv('results/quotes.csv')\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        scraper.generate_report()\n",
    "        \n",
    "        print(f\"\\n✓ Web scraping completed successfully!\")\n",
    "        print(f\"✓ {len(quotes)} quotes extracted and saved\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n✗ Web scraping failed\")\n",
    "        print(\"\\nTroubleshooting suggestions:\")\n",
    "        print(\"1. Check if the URL is accessible in your browser\")\n",
    "        print(\"2. Verify the website structure hasn't changed\")\n",
    "        print(\"3. Try using a VPN if there are geographical restrictions\")\n",
    "        print(\"4. Check the website's robots.txt file\")\n",
    "        print(\"5. Consider using Selenium for JavaScript-heavy sites\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
