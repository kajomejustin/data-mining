<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Introduction to Statistical Learning - Comprehensive Summary</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #2c3e50;
            min-height: 100vh;
            padding: 20px;
            line-height: 1.6;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #3498db, #2980b9);
            color: white;
            padding: 30px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }
        
        .header::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: repeating-linear-gradient(
                45deg,
                transparent,
                transparent 10px,
                rgba(255,255,255,0.1) 10px,
                rgba(255,255,255,0.1) 20px
            );
            animation: slide 20s linear infinite;
        }
        
        @keyframes slide {
            0% { transform: translate(-50%, -50%) rotate(0deg); }
            100% { transform: translate(-50%, -50%) rotate(360deg); }
        }
        
        .header-content {
            position: relative;
            z-index: 2;
        }
        
        .university-info {
            background: rgba(255,255,255,0.2);
            padding: 15px;
            border-radius: 10px;
            margin-bottom: 20px;
        }
        
        .university-info h2 {
            font-size: 1.8em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        .course-details {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin-top: 15px;
        }
        
        .detail-item {
            background: rgba(255,255,255,0.1);
            padding: 10px;
            border-radius: 8px;
            border-left: 4px solid #f39c12;
        }
        
        .detail-item strong {
            display: block;
            font-size: 0.9em;
            opacity: 0.8;
        }
        
        .detail-item span {
            font-size: 1.1em;
            font-weight: 600;
        }
        
        .main-content {
            padding: 40px;
        }
        
        .book-title {
            text-align: center;
            font-size: 3em;
            color: #2c3e50;
            margin-bottom: 30px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
            position: relative;
        }
        
        .book-title::after {
            content: '';
            position: absolute;
            bottom: -10px;
            left: 50%;
            transform: translateX(-50%);
            width: 100px;
            height: 4px;
            background: linear-gradient(90deg, #3498db, #e74c3c);
            border-radius: 2px;
        }
        
        .book-overview {
            background: linear-gradient(135deg, #f8f9fa, #e9ecef);
            border: 2px solid #3498db;
            border-radius: 15px;
            padding: 25px;
            margin-bottom: 40px;
            position: relative;
            overflow: hidden;
        }
        
        .book-overview::before {
            content: '"';
            position: absolute;
            top: -10px;
            left: 20px;
            font-size: 4em;
            color: #3498db;
            opacity: 0.3;
        }
        
        .book-overview p {
            font-size: 1.2em;
            font-style: italic;
            text-align: center;
            color: #34495e;
            margin: 0;
        }
        
        .book-meta {
            display: flex;
            justify-content: center;
            gap: 30px;
            margin-top: 15px;
            flex-wrap: wrap;
        }
        
        .meta-item {
            background: white;
            padding: 10px 20px;
            border-radius: 25px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            border: 2px solid #3498db;
        }
        
        .section-title {
            font-size: 2.5em;
            color: #2980b9;
            text-align: center;
            margin: 50px 0 30px;
            text-transform: uppercase;
            letter-spacing: 2px;
            position: relative;
        }
        
        .section-title::before,
        .section-title::after {
            content: '';
            position: absolute;
            top: 50%;
            width: 50px;
            height: 3px;
            background: #e74c3c;
        }
        
        .section-title::before {
            left: -70px;
        }
        
        .section-title::after {
            right: -70px;
        }
        
        .toc-section {
            background: white;
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 40px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }
        
        .toc-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }
        
        .toc-table th {
            background: linear-gradient(135deg, #3498db, #2980b9);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        .toc-table td {
            padding: 15px;
            border-bottom: 1px solid #ecf0f1;
            transition: background-color 0.3s ease;
        }
        
        .toc-table tr:hover td {
            background-color: #f8f9fa;
        }
        
        .toc-table tr:nth-child(even) {
            background-color: #fbfcfd;
        }
        
        .chapter-number {
            font-weight: bold;
            color: #e74c3c;
            font-size: 1.2em;
        }
        
        .chapters-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(500px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }
        
        .chapter-card {
            background: white;
            border-radius: 15px;
            padding: 25px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            border-left: 5px solid #3498db;
            position: relative;
            overflow: hidden;
        }
        
        .chapter-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 3px;
            background: linear-gradient(90deg, #3498db, #e74c3c, #f39c12);
        }
        
        .chapter-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 40px rgba(0,0,0,0.2);
        }
        
        .chapter-header {
            display: flex;
            align-items: center;
            margin-bottom: 20px;
        }
        
        .chapter-icon {
            width: 50px;
            height: 50px;
            background: linear-gradient(135deg, #3498db, #2980b9);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
            font-size: 1.2em;
            margin-right: 15px;
        }
        
        .chapter-title {
            font-size: 1.4em;
            color: #2c3e50;
            margin: 0;
        }
        
        .chapter-content {
            margin-bottom: 20px;
        }
        
        .content-item {
            margin-bottom: 6px;
            padding: 6px 8px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 3px solid #3498db;
            
        }
        
        .content-item strong {
            color: #2980b9;
            font-weight: 600;
        }
        
        .github-link {
            display: inline-block;
            padding: 12px 25px;
            background: linear-gradient(135deg, #e74c3c, #c0392b);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 1px;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(231,76,60,0.3);
        }
        
        .github-link:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(231,76,60,0.4);
            background: linear-gradient(135deg, #c0392b, #a93226);
        }
        
        .github-link::before {
            content: 'üîó ';
            margin-right: 5px;
        }
        
        .algorithms-section {
            background: white;
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 40px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }
        
        .algorithms-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 25px;
            margin-top: 30px;
        }
        
        .algorithm-category {
            background: linear-gradient(135deg, #f8f9fa, #e9ecef);
            border-radius: 12px;
            padding: 20px;
            border-left: 5px solid #e74c3c;
        }
        
        .algorithm-category h3 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.3em;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        
        .algorithm-category ul {
            list-style: none;
            padding: 0;
        }
        
        .algorithm-category li {
            background: white;
            margin: 8px 0;
            padding: 10px 15px;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            border-left: 3px solid #3498db;
            font-weight: 500;
        }
        
        .algorithm-category li::before {
            content: '‚ñ∂ ';
            color: #3498db;
            font-weight: bold;
        }
        
        .footer {
            background: linear-gradient(135deg, #2c3e50, #34495e);
            color: white;
            text-align: center;
            padding: 30px;
            margin-top: 50px;
        }
        
        .footer p {
            font-size: 1.1em;
            margin: 0;
        }
        
        .footer::before {
            content: 'üìö ';
            font-size: 2em;
            display: block;
            margin-bottom: 10px;
        }
        
        @media (max-width: 768px) {
            .book-title {
                font-size: 2em;
            }
            
            .chapters-grid {
                grid-template-columns: 1fr;
            }
            
            .main-content {
                padding: 20px;
            }
            
            .section-title::before,
            .section-title::after {
                display: none;
            }
            
            .course-details {
                grid-template-columns: 1fr;
            }
        }
        
        .highlight {
            background: linear-gradient(120deg, #a8edea 0%, #fed6e3 100%);
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: 600;
        }
        
        .badge {
            display: inline-block;
            padding: 4px 12px;
            background: #3498db;
            color: white;
            border-radius: 15px;
            font-size: 0.8em;
            font-weight: 600;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="header">
            <div class="header-content">
                <div class="university-info">
                    <h2>üéì Adventist University of Central Africa</h2>
                    <div class="course-details">
                        <div class="detail-item">
                            <strong>Program:</strong>
                            <span>Msc in Big Data Analytics</span>
                        </div>
                        <div class="detail-item">
                            <strong>Module:</strong>
                            <span>MSDA9223 Data Mining and Information Retrieval</span>
                        </div>
                        <div class="detail-item">
                            <strong>Student:</strong>
                            <span>101002 Justin Tuyisenge</span>
                        </div>
                        <div class="detail-item">
                            <strong>Instructor:</strong>
                            <span>Pacifique Nizeyimana, PhD</span>
                        </div>
                    </div>
                </div>
            </div>
        </header>

        <main class="main-content">
            <h1 class="book-title">Introduction to Statistical Learning</h1>
            
            <!-- Book Overview Section -->
            <section class="book-overview">
                <p><strong>Introduction to Statistical Learning with Python</strong> by <em>Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani</em>, offers a comprehensive exploration of statistical learning methods. This essential text covers both <span class="highlight">supervised and unsupervised learning</span> techniques, providing practical implementations using <strong>Python</strong> for real-world data analysis and machine learning applications.</p>
                
                <div class="book-meta">
                    <div class="meta-item">
                        <strong>üìÖ Publication:</strong> July 5, 2023
                    </div>
                    <div class="meta-item">
                        <strong>üî¨ Focus:</strong> Statistical Learning Methods
                    </div>
                    <div class="meta-item">
                        <strong>üíª Language:</strong> Python Implementation
                    </div>
                </div>
            </section>

            <!-- Table of Contents -->
            <h3 class="section-title">üìã Table of Contents</h3>
                <section class="toc-section">
                    <p><strong>The following chapters were covered during our coursework:</strong></p>
                    <table class="toc-table">
                        <thead>
                            <tr>
                                <th>Chapter</th>
                                <th>Title</th>
                                <th>Focus Area</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td class="chapter-number">3</td>
                                <td>Linear Regression</td>
                                <td><span class="badge">Supervised</span></td>
                            </tr>
                            <tr>
                                <td class="chapter-number">4</td>
                                <td>Classification</td>
                                <td><span class="badge">Supervised</span></td>
                            </tr>
                            <tr>
                                <td class="chapter-number">5</td>
                                <td>Resampling Methods</td>
                                <td><span class="badge">Validation</span></td>
                            </tr>
                            <tr>
                                <td class="chapter-number">6</td>
                                <td>Linear Model Selection</td>
                                <td><span class="badge">Regularization</span></td>
                            </tr>
                            <tr>
                                <td class="chapter-number">8</td>
                                <td>Tree-Based Methods</td>
                                <td><span class="badge">Ensemble</span></td>
                            </tr>
                            <tr>
                                <td class="chapter-number">9</td>
                                <td>Support Vector Machines</td>
                                <td><span class="badge">Classification</span></td>
                            </tr>
                            <tr>
                                <td class="chapter-number">10</td>
                                <td>Deep Learning</td>
                                <td><span class="badge">Neural Networks</span></td>
                            </tr>
                            <tr>
                                <td class="chapter-number">12</td>
                                <td>Unsupervised Learning</td>
                                <td><span class="badge">Unsupervised</span></td>
                            </tr>
                        </tbody>
                    </table>
                </section>

            <!-- Chapter Summaries -->
            <h2 class="section-title">üìö Summary by Chapter </h2>
            <section class="chapters-grid">
                <article class="chapter-card">
                    <div class="chapter-header">
                        <div class="chapter-icon">3</div>
                        <h3 class="chapter-title">Linear Regression</h3>
                    </div>
                    <div class="chapter-content">
                        <div class="content-item">
                            <strong>üéØ Main Ideas:</strong> Modeling linear relationships between predictors and continuous responses using least squares estimation.
                        </div>
                        <div class="content-item">
                            <strong>üîß Key Techniques:</strong> Simple and multiple linear regression, least squares estimation, hypothesis testing, confidence intervals, variable selection methods.
                        </div>
                        <div class="content-item">
                            <strong>üí° Key Takeaways:</strong> Linear regression assumptions must be validated through residual analysis and diagnostic plots for reliable predictions.
                        </div>
                    </div>
                    <a href="https://github.com/kajomejustin/data-mining/blob/dm/Labs/Ch03-linreg-lab.ipynb" class="github-link">View Notebook</a>
                </article>

                <article class="chapter-card">
                    <div class="chapter-header">
                        <div class="chapter-icon">4</div>
                        <h3 class="chapter-title">Classification</h3>
                    </div>
                    <div class="chapter-content">
                        <div class="content-item">
                            <strong>üéØ Main Ideas:</strong> Predicting categorical outcomes using various classification algorithms and decision boundaries.
                        </div>
                        <div class="content-item">
                            <strong>üîß Key Techniques:</strong> Logistic regression, Linear and Quadratic Discriminant Analysis (LDA/QDA), K-Nearest Neighbors (KNN), confusion matrices.
                        </div>
                        <div class="content-item">
                            <strong>üí° Key Takeaways:</strong> Algorithm selection depends on data characteristics, sample size, and the assumption of class distributions.
                        </div>
                    </div>
                    <a href="https://github.com/kajomejustin/data-mining/blob/dm/Labs/Ch04-classification-lab.ipynb" class="github-link">View Notebook</a>
                </article>

                <article class="chapter-card">
                    <div class="chapter-header">
                        <div class="chapter-icon">5</div>
                        <h3 class="chapter-title">Resampling Methods</h3>
                    </div>
                    <div class="chapter-content">
                        <div class="content-item">
                            <strong>üéØ Main Ideas:</strong> Enhancing model evaluation and selection through statistical resampling techniques.
                        </div>
                        <div class="content-item">
                            <strong>üîß Key Techniques:</strong> K-fold cross-validation, leave-one-out cross-validation (LOOCV), bootstrap sampling, validation set approach.
                        </div>
                        <div class="content-item">
                            <strong>üí° Key Takeaways:</strong> Resampling methods provide robust estimates of model performance and help prevent overfitting.
                        </div>
                    </div>
                    <a href="https://github.com/kajomejustin/data-mining/blob/dm/Labs/Ch05-resample-lab.ipynb" class="github-link">View Notebook</a>
                </article>

                <article class="chapter-card">
                    <div class="chapter-header">
                        <div class="chapter-icon">6</div>
                        <h3 class="chapter-title">Linear Model Selection</h3>
                    </div>
                    <div class="chapter-content">
                        <div class="content-item">
                            <strong>üéØ Main Ideas:</strong> Optimizing model complexity through feature selection and regularization techniques.
                        </div>
                        <div class="content-item">
                            <strong>üîß Key Techniques:</strong> Best subset selection, forward/backward stepwise selection, Ridge regression, Lasso regression, Principal Component Regression (PCR).
                        </div>
                        <div class="content-item">
                            <strong>üí° Key Takeaways:</strong> Regularization methods help balance bias-variance tradeoff and prevent overfitting in high-dimensional datasets.
                        </div>
                    </div>
                    <a href="https://github.com/kajomejustin/data-mining/blob/dm/Labs/Ch06-varselect-lab.ipynb" class="github-link">View Notebook</a>
                </article>

                <article class="chapter-card">
                    <div class="chapter-header">
                        <div class="chapter-icon">8</div>
                        <h3 class="chapter-title">Tree-Based Methods</h3>
                    </div>
                    <div class="chapter-content">
                        <div class="content-item">
                            <strong>üéØ Main Ideas:</strong> Decision trees and powerful ensemble methods for both regression and classification tasks.
                        </div>
                        <div class="content-item">
                            <strong>üîß Key Techniques:</strong> Decision trees, bagging, random forests, gradient boosting, pruning techniques.
                        </div>
                        <div class="content-item">
                            <strong>üí° Key Takeaways:</strong> Ensemble methods significantly improve predictive performance by combining multiple weak learners.
                        </div>
                    </div>
                    <a href="https://github.com/kajomejustin/data-mining/blob/dm/Labs/Ch08-baggboost-lab.ipynb" class="github-link">View Notebook</a>
                </article>

                <article class="chapter-card">
                    <div class="chapter-header">
                        <div class="chapter-icon">9</div>
                        <h3 class="chapter-title">Support Vector Machines</h3>
                    </div>
                    <div class="chapter-content">
                        <div class="content-item">
                            <strong>üéØ Main Ideas:</strong> Maximizing classification margins and handling non-linear decision boundaries using kernel methods.
                        </div>
                        <div class="content-item">
                            <strong>üîß Key Techniques:</strong> Support Vector Classifier, Support Vector Machine (SVM), kernel tricks (linear, polynomial, radial), soft margin classification.
                        </div>
                        <div class="content-item">
                            <strong>üí° Key Takeaways:</strong> SVMs are particularly effective for high-dimensional data and can handle complex decision boundaries through kernel transformations.
                        </div>
                    </div>
                    <a href="https://github.com/kajomejustin/data-mining/blob/dm/Labs/Ch09-svm-lab.ipynb" class="github-link">View Notebook</a>
                </article>

                <article class="chapter-card">
                    <div class="chapter-header">
                        <div class="chapter-icon">10</div>
                        <h3 class="chapter-title">Deep Learning</h3>
                    </div>
                    <div class="chapter-content">
                        <div class="content-item">
                            <strong>üéØ Main Ideas:</strong> Advanced neural network architectures for complex pattern recognition and representation learning.
                        </div>
                        <div class="content-item">
                            <strong>üîß Key Techniques:</strong> Multilayer perceptrons, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), backpropagation, dropout regularization.
                        </div>
                        <div class="content-item">
                            <strong>üí° Key Takeaways:</strong> Deep learning models excel at capturing complex patterns but require large datasets and careful regularization.
                        </div>
                    </div>
                    <a href="https://github.com/kajomejustin/data-mining/blob/dm/Labs/Ch10-deeplearning-lab.ipynb" class="github-link">View Notebook</a>
                </article>

                <article class="chapter-card">
                    <div class="chapter-header">
                        <div class="chapter-icon">12</div>
                        <h3 class="chapter-title">Unsupervised Learning</h3>
                    </div>
                    <div class="chapter-content">
                        <div class="content-item">
                            <strong>üéØ Main Ideas:</strong> Discovering hidden patterns and structures in unlabeled data without target variables.
                        </div>
                        <div class="content-item">
                            <strong>üîß Key Techniques:</strong> Principal Component Analysis (PCA), K-means clustering, hierarchical clustering, dimensionality reduction techniques.
                        </div>
                        <div class="content-item">
                            <strong>üí° Key Takeaways:</strong> Unsupervised learning reveals hidden data structures and is essential for exploratory data analysis.
                        </div>
                    </div>
                    <a href="https://github.com/kajomejustin/data-mining/blob/dm/Labs/Ch12-unsup-lab.ipynb" class="github-link">View Notebook</a>
                </article>
            </section>

            <!-- Algorithms Section -->
            <h2 class="section-title">üîç Algorithms by Task</h2>
<section class="algorithms-section">
    <p><strong>Key algorithms covered in the coursework for regression, classification, and unsupervised learning tasks:</strong></p>
    <table class="toc-table">
        <thead>
            <tr>
                <th>Algorithm</th>
                <th>Chapter</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td colspan="3" style="font-weight: bold; background: #f8f9fa; text-align: center;">Regression Algorithms</td>
            </tr>
            <tr>
                <td>Linear Regression</td>
                <td class="chapter-number">3</td>
                <td class="content-item">Models linear relationships using least squares estimation.</td>
            </tr>
            <tr>
                <td>Ridge Regression</td>
                <td class="chapter-number">6</td>
                <td class="content-item">Regularized linear regression to handle multicollinearity and overfitting.</td>
            </tr>
            <tr>
                <td>Lasso Regression</td>
                <td class="chapter-number">6</td>
                <td class="content-item">Regularized regression with feature selection via L1 penalty.</td>
            </tr>
            <tr>
                <td>Principal Component Regression (PCR)</td>
                <td class="chapter-number">6</td>
                <td class="content-item">Combines PCA with regression for dimensionality reduction.</td>
            </tr>
            <tr>
                <td>Decision Trees</td>
                <td class="chapter-number">8</td>
                <td class="content-item">Non-linear regression using tree-based splits.</td>
            </tr>
            <tr>
                <td>Random Forests</td>
                <td class="chapter-number">8</td>
                <td class="content-item">Ensemble of decision trees for robust regression.</td>
            </tr>
            <tr>
                <td>Gradient Boosting</td>
                <td class="chapter-number">8</td>
                <td class="content-item">Sequential tree-based ensemble for improved regression performance.</td>
            </tr>
            <tr>
                <td colspan="3" style="font-weight: bold; background: #f8f9fa; text-align: center;">Classification Algorithms</td>
            </tr>
            <tr>
                <td>Logistic Regression</td>
                <td class="chapter-number">4</td>
                <td class="content-item">Predicts categorical outcomes using a logistic function.</td>
            </tr>
            <tr>
                <td>Linear Discriminant Analysis (LDA)</td>
                <td class="chapter-number">4</td>
                <td class="content-item">Assumes normal class distributions for classification.</td>
            </tr>
            <tr>
                <td>Quadratic Discriminant Analysis (QDA)</td>
                <td class="chapter-number">4</td>
                <td class="content-item">Relaxes LDA‚Äôs covariance assumptions.</td>
            </tr>
            <tr>
                <td>K-Nearest Neighbors (KNN)</td>
                <td class="chapter-number">4</td>
                <td class="content-item">Non-parametric classification based on nearest neighbors.</td>
            </tr>
            <tr>
                <td>Decision Trees</td>
                <td class="chapter-number">8</td>
                <td class="content-item">Splits data into regions for classification.</td>
            </tr>
            <tr>
                <td>Random Forests</td>
                <td class="chapter-number">8</td>
                <td class="content-item">Ensemble of decision trees for robust classification.</td>
            </tr>
            <tr>
                <td>Gradient Boosting</td>
                <td class="chapter-number">8</td>
                <td class="content-item">Sequential ensemble for high-accuracy classification.</td>
            </tr>
            <tr>
                <td>Support Vector Classifier</td>
                <td class="chapter-number">9</td>
                <td class="content-item">Linear classification with maximum margin.</td>
            </tr>
            <tr>
                <td>Support Vector Machine (SVM)</td>
                <td class="chapter-number">9</td>
                <td class="content-item">Non-linear classification using kernel tricks.</td>
            </tr>
            <tr>
                <td colspan="3" style="font-weight: bold; background: #f8f9fa; text-align: center;">Unsupervised Learning Algorithms</td>
            </tr>
            <tr>
                <td>Principal Component Analysis (PCA)</td>
                <td class="chapter-number">12</td>
                <td class="content-item">Dimensionality reduction via orthogonal transformations.</td>
            </tr>
            <tr>
                <td>K-means Clustering</td>
                <td class="chapter-number">12</td>
                <td class="content-item">Partitions data into K clusters based on centroids.</td>
            </tr>
            <tr>
                <td>Hierarchical Clustering</td>
                <td class="chapter-number">12</td>
                <td class="content-item">Builds a hierarchy of clusters using agglomerative or divisive methods.</td>
            </tr>
        </tbody>
    </table>
</section>
        </main>

        <footer class="footer">
            <p>Submitted by Justin Tuyisenge | GitHub Repository: <a href="https://github.com/kajomejustin" style="color: #3498db;">View Repository</a></p>
            <p>Hosted on GitHub Pages: <a href="https://github.com/kajomejustin/data-mining/tree/dm/Labs" style="color: #3498db;">View Page</a></p>
        </footer>
    </div>
</body>
</html>