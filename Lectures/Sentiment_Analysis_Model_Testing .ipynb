{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCXLU2zeUYv7"
      },
      "source": [
        "##Import your libraries for sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFDjBXQEvOnh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install nltk\n",
        "!python -m nltk.downloader punkt\n",
        "!pip install pandas\n",
        "!pip install pyLDAvis\n",
        "!pip install sklearn\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sooh_rY5vjvV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import sklearn.metrics as skm\n",
        "import sklearn.cluster as skc\n",
        "import sklearn.preprocessing as skp\n",
        "import matplotlib.pyplot as plt\n",
        "import altair as alt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import networkx as nx\n",
        "\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import scipy as sp\n",
        "import scipy.spatial.distance as spd\n",
        "import scipy.cluster.hierarchy as sph\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_IQStv4Umnq"
      },
      "source": [
        "##Load your scrapped Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK60wdjnPd15"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    df1 = pd.read_csv('/content/CAT.csv')\n",
        "    df2 = pd.read_csv('/content/ETN.csv')\n",
        "    df3 = pd.read_csv('/content/GE.csv')\n",
        "\n",
        "    # Concatenate the dataframes\n",
        "    df = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "\n",
        "    # Print some info about the concatenated dataframe (optional)\n",
        "    print(df.info())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"One or more data files not found. Please check the filenames and paths.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(\"One or more data files are empty.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_8KrsH5QeDM"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChktE55NxtRe"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_csv(\"/content/Manufacturing.csv\")\n",
        "#df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWoba3tq0HTG"
      },
      "outputs": [],
      "source": [
        "df['Comment'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQXtkuuBVTHA"
      },
      "source": [
        "##Load Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3rJ-wfK0R_-"
      },
      "outputs": [],
      "source": [
        "# Load stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Show stop words\n",
        "stop_words[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv3HPZVNVYBf"
      },
      "source": [
        "##Perform data cleaning by removing hastags, urls, mentions, symbols, www, pictures, emoji, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRiDOS4o0W_7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Removing hashtags and mentions\n",
        "def get_hashtags(text):\n",
        "    hashtags = re.findall(r'\\#\\w+',text.lower())\n",
        "    return hashtags\n",
        "def get_mentions(text):\n",
        "    mentions = re.findall(r'\\@\\w+',text.lower())\n",
        "    return mentions\n",
        "\n",
        "# Cleaning up the text of the tweets\n",
        "def remove_content(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text) #remove urls\n",
        "    text=re.sub(r'\\S+\\.com\\S+','',text) #remove urls\n",
        "    text=re.sub(r'\\@\\w+','',text) #remove mentions\n",
        "    text =re.sub(r'\\#\\w+','',text) #remove hashtags\n",
        "    return text\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    \"\"\"\n",
        "    tweets cleaning by\n",
        "    1) lowering the case of the tweet,\n",
        "    2) removing unwanted symbols and replacing them with a whitespace,\n",
        "    3) split sentences into words according to whitespaces and then\n",
        "    4) join back with a single whitespace as separator between various words\n",
        "    \"\"\"\n",
        "    return \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",tweet.lower()).split())\n",
        "\n",
        "def process_text(text, stem=False): #clean text\n",
        "    text=remove_content(text)\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    text = re.sub('[^A-Za-z]', ' ', text.lower()) #remove non-alphabets\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', str(text)) # remove @mentions\n",
        "    text = re.sub(r'#', '',  str(text)) # remove the '#' symbol\n",
        "    text = re.sub(r'RT[\\s]+', '',  str(text)) # remove RT\n",
        "    text = re.sub(r'https?\\/\\/S+', '',  str(text)) # remove the hyperlink\n",
        "    text = re.sub(r'http\\S+', '',  str(text)) # remove the hyperlink\n",
        "    text = re.sub(r'www\\S+', '',  str(text)) # remove the www\n",
        "    text = re.sub(r'pic+', '',  str(text)) # remove the pic\n",
        "    text = re.sub(r'com', '',  str(text)) # remove the pic\n",
        "    text = re.sub(r\"\\bamp\\b\", ' ', text.lower()) #remove \"amp\" which is coming from the translation of &\n",
        "    text = re.sub(r\"\\bco\\b\", ' ', text.lower()) #remove \"co\" which was one of the top words found below\n",
        "    tokenized_text = word_tokenize(text) #tokenize\n",
        "    #tokenized_text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
        "    clean_text = [\n",
        "         word for word in tokenized_text\n",
        "         if (word not in stop_words and len(word)>1)\n",
        "    ]\n",
        "    if stem:\n",
        "        clean_text=[stemmer.stem(word) for word in clean_text]\n",
        "    clean_text = [lemmatizer.lemmatize(word) for word in clean_text]\n",
        "    return ' '.join(clean_text)\n",
        "\n",
        "#functions used to remove search terms from all the tweets\n",
        "#function to remove duplicates from a string - in this case the string is the keywords used to scrape the tweets\n",
        "def removeDupWithoutOrder(string):\n",
        "    words = string.lower().split()\n",
        "    return \" \".join(sorted(set(words), key=words.index)).replace('OR', '').replace('  ', ' ')\n",
        "\n",
        "#function to search for string i.e. remove specific words (search_terms in this case)\n",
        "def remove_search(text, search_terms):\n",
        "    query = text.lower()\n",
        "    querywords = query.split()\n",
        "    resultwords  = [word for word in querywords if word.lower() not in search_terms]\n",
        "    return ' '.join(resultwords)\n",
        "\n",
        "# define function to plot frequency of bi-grams, tri-grams, single words, phrases etc\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "def plot_topn(sentences, ngram_range=(1,3), top=20,firstword=''):\n",
        "    c=CountVectorizer(ngram_range=ngram_range)\n",
        "    X=c.fit_transform(sentences)\n",
        "    words=pd.DataFrame(X.sum(axis=0),columns=c.get_feature_names()).T.sort_values(0,ascending=False).reset_index()\n",
        "    res=words[words['index'].apply(lambda x: firstword in x)].head(top)\n",
        "    pl=px.bar(res, x='index',y=0)\n",
        "    pl.update_layout(yaxis_title='count',xaxis_title='Phrases')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psa3LkqL0bnL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "re.compile('<title>(.*)</title>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-5hoccY0ee4"
      },
      "outputs": [],
      "source": [
        "# removing useless content (hashtags, mentions)\n",
        "df['Comment'].apply(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV2yCSb7JImW"
      },
      "outputs": [],
      "source": [
        "df['cleaned_comments'] = df['Comment'].astype(str).apply(lambda x: process_tweet(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siL1uxoJ0slV"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xysOgEfvWJ5b"
      },
      "source": [
        "###Check the number of comments or reviews in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHhtHmSY1JF5"
      },
      "outputs": [],
      "source": [
        "df['cleaned_comments'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiVmsMwscauT"
      },
      "outputs": [],
      "source": [
        "#Save cleaned_comments data\n",
        "df.to_csv('cleaned_comments.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BduGUEbvOxEh"
      },
      "outputs": [],
      "source": [
        "df['Comment'] = df['Comment'].str.replace('http\\S+', '')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCvEB0bWWhe5"
      },
      "source": [
        "###Generate Word Frequency to analyze the most occuring word within the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bULuaDYR1P7W"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import urllib\n",
        "import requests\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn_MDe4a1UiG"
      },
      "outputs": [],
      "source": [
        "comment_words = ''\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "# iterate through the csv file\n",
        "for val in df.cleaned_comments:\n",
        "\n",
        "    # typecaste each val to string\n",
        "    val = str(val)\n",
        "\n",
        "    # split the value\n",
        "    tokens = val.split()\n",
        "\n",
        "    # Converts each token into lowercase\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i].lower()\n",
        "\n",
        "    comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "wordcloud = WordCloud(width = 1000, height = 800,\n",
        "                background_color='black', colormap='Dark2',\n",
        "                collocations=False,\n",
        "                stopwords = stopwords,\n",
        "                min_font_size = 12).generate(comment_words)\n",
        "\n",
        "# plot the WordCloud image\n",
        "plt.figure(figsize = (10, 10), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEbxD7MnW6BW"
      },
      "source": [
        "###Plot Polarity and Subjectivity Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4rwN0Iu3O9V"
      },
      "outputs": [],
      "source": [
        "def getSubjectivity(text):\n",
        "    return TextBlob( str(text)).sentiment.subjectivity\n",
        "\n",
        "def getPolarity(text):\n",
        "    return TextBlob( str(text)).sentiment.polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqgZPcFp3Stg"
      },
      "outputs": [],
      "source": [
        "df.dropna(subset=['cleaned_comments'], inplace = True)\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSS3Ttxr3WIO"
      },
      "outputs": [],
      "source": [
        "df['Subjectivity'] = df['cleaned_comments'].apply(getSubjectivity)\n",
        "df['Polarity'] = df['cleaned_comments'].apply(getPolarity)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeCtfpki3dXT"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_Polarity_Analysis(score):\n",
        "    if score < 0:\n",
        "      return 'Negative'\n",
        "    elif score == 0:\n",
        "      return 'Neutral'\n",
        "    else:\n",
        "      return 'Positive'\n",
        "def get_Subjectivity_Analysis(score):\n",
        "    if score >  0:\n",
        "      return 'Opinion'\n",
        "    else:\n",
        "      return 'Fact'\n",
        "\n",
        "df['Analysis_Polarity'] = df['Polarity'].apply(get_Polarity_Analysis)\n",
        "\n",
        "df['Analysis_Subjectivity'] = df['Subjectivity'].apply(get_Subjectivity_Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeRwWxtZ3gwr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sns.set(font_scale=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIiP61oBXrip"
      },
      "source": [
        "###Plot Polarity and Subjectiviy Score in Scatter Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YeYuG3l3jFA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(13,8))\n",
        "\n",
        "plt.scatter(df['Polarity'], df['Subjectivity'], c=df['Polarity'], s=100, cmap='Spectral')\n",
        "\n",
        "plt.xlim(-1.1, 1.1)\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Subjectivity')\n",
        "plt.show(),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONVZx_NOcbMQ"
      },
      "outputs": [],
      "source": [
        "df['Sentiment'] = df.apply(lambda x: ' '.join([str(x['Analysis_Polarity']),str(x['Analysis_Subjectivity'])]),axis=1)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p90woJTSX6lZ"
      },
      "source": [
        "###Plot Polarity Score for the entire dataset using bar chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W2gqMCo3pd3"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.title('Polarity Sentiment Analysis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Counts')\n",
        "df['Analysis_Polarity'].value_counts().plot(kind = 'bar',  color=sns.palettes.mpl_palette('rocket'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "533qtzsEYGvJ"
      },
      "source": [
        "###Plot Subjectivity and Objective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAsHuOrc4sBs"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.title('Subjectivity Sentiment Analysis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Counts')\n",
        "df['Analysis_Subjectivity'].value_counts().plot(kind = 'bar',  color=sns.palettes.mpl_palette('rocket'))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwVwFFziDk7l"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-Sf59AfH7vI"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvhWd7amIbdE"
      },
      "outputs": [],
      "source": [
        "\n",
        "pyLDAvis.enable_notebook()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU0e_x66EFjx"
      },
      "outputs": [],
      "source": [
        "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
        "                                stop_words = 'english',\n",
        "                                lowercase = True,\n",
        "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
        "                                max_df = 0.5,\n",
        "                                min_df = 10)\n",
        "dtm_tf = tf_vectorizer.fit_transform(df['cleaned_comments'].values.astype('U'))\n",
        "print(dtm_tf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUwv3LmAEZoS"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
        "dtm_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_comments'].values.astype('U'))\n",
        "print(dtm_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8WKG_G9Exq2"
      },
      "outputs": [],
      "source": [
        "# for TF DTM\n",
        "lda_tf = LatentDirichletAllocation(n_components =10, random_state=50)\n",
        "lda_tf.fit(dtm_tf)\n",
        "# for TFIDF DTM\n",
        "lda_tfidf = LatentDirichletAllocation(n_components =10, random_state=50)\n",
        "lda_tfidf.fit(dtm_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxhTdWRCFQ76"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxkOVtJBYQYT"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.lda_model\n",
        "...\n",
        "pyLDAvis.lda_model.prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHOYXewujKVj"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords  # Import the stopwords module\n",
        "\n",
        "english_stopwords = stopwords.words('english')  # Call words on the module\n",
        "english_stopwords.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sentences_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuation.\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in english_stopwords] for doc in texts]\n",
        "\n",
        "comments = df.cleaned_comments.to_list()\n",
        "comment_words = list(sentences_to_words(comments))\n",
        "\n",
        "# Remove stop words.\n",
        "comment_words = remove_stopwords(comment_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NKiRn4K79os"
      },
      "outputs": [],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary.\n",
        "id2word = corpora.Dictionary(comment_words)\n",
        "\n",
        "# Create Corpus.\n",
        "texts = comment_words\n",
        "\n",
        "# Term Document Frequency.\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPjPd8nE8GB5"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Number of topics.\n",
        "num_topics = 10\n",
        "\n",
        "# Build LDA model.\n",
        "lda_model = gensim.models.LdaMulticore(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=num_topics\n",
        ")\n",
        "\n",
        "# Print the Keyword in the 10 topics.\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR15Ugb08RV5"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Visualize the topics.\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join(f'cleaned_comments_lda_{num_topics}.data')\n",
        "\n",
        "if os.path.exists(LDAvis_data_filepath):\n",
        "    # Load the pre-prepared pyLDAvis data from disk.\n",
        "    with open(LDAvis_data_filepath, 'rb') as f:\n",
        "        LDAvis_prepared = pickle.load(f)\n",
        "else:\n",
        "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, f'{LDAvis_data_filepath}.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4wZThFH-itK"
      },
      "outputs": [],
      "source": [
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJnqTXdICDI0"
      },
      "outputs": [],
      "source": [
        "topic_values = lda_tf.transform(dtm_tf)\n",
        "topic_values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg7XPPAjCG4B"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tje6FjL3CYpU"
      },
      "outputs": [],
      "source": [
        "df_1=df.replace({0:' Insider Trading',1:'Stock Performance',2:'AI & Data Centers',3:'Price Targets',4:'Energy & Power Sector',5:'Defense Collaborations',6:'Stock Disclosures',7:'Chinese Market',8:'Company Acquisitions',9:'Financial Reports'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHBIxSncD9Mg"
      },
      "outputs": [],
      "source": [
        "df_1['topic'] = topic_values.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HFxTEEvEDo5"
      },
      "outputs": [],
      "source": [
        "df['Topic'] = topic_values.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr_aytCwEIdR"
      },
      "outputs": [],
      "source": [
        "df_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSJp3cd_EM_X"
      },
      "outputs": [],
      "source": [
        "df_1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ksl0VDxESAn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(40,25))\n",
        "\n",
        "g=sns.lmplot(x=\"Polarity\", y=\"Subjectivity\", hue='Topic', data=df, fit_reg=False, legend=False,palette=\"GnBu_d\", col='Topic', legend_out=True)\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhfoHbbAEW4_"
      },
      "outputs": [],
      "source": [
        "df_2 = df_1.groupby(['Topic'])['Analysis_Polarity'].value_counts().unstack('Topic').transpose()\n",
        "\n",
        "df_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HHSqSI6EbJk"
      },
      "outputs": [],
      "source": [
        "df_2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut91oiFiEffO"
      },
      "outputs": [],
      "source": [
        "df_2['Total'] = df_2.sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXST__TNEjme"
      },
      "outputs": [],
      "source": [
        "df_2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TURhJeKqEoOR"
      },
      "outputs": [],
      "source": [
        "for i in df_2:\n",
        "    df_2[i] = round(df_2[i]*100/df_2.Total)\n",
        "\n",
        "df_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUHMS-wDGDPj"
      },
      "outputs": [],
      "source": [
        "# Plot and visualizing the counts for each topic\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.title('Topic Analysis')\n",
        "plt.xlabel('Topic')\n",
        "plt.ylabel('Counts')\n",
        "df_1['Topic'].value_counts().plot(kind = 'bar',  color=sns.palettes.mpl_palette('flare'))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QlOEr3nEyCT"
      },
      "outputs": [],
      "source": [
        "df_2=df_2.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4h3sIaFOFo6"
      },
      "outputs": [],
      "source": [
        "df_2['Total'] = df_2.sum(axis=1)\n",
        "df_2 = df_2.drop(['Total'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1-yigzmHdsL"
      },
      "outputs": [],
      "source": [
        "print(df_2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwdpPJAgHeCV"
      },
      "outputs": [],
      "source": [
        "print(df_2.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb_BqCEP4_Kt"
      },
      "outputs": [],
      "source": [
        "print(df_2.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMVmVB9BT0LB"
      },
      "outputs": [],
      "source": [
        "# Create a new dataframe with topic names and polarity percentages\n",
        "df_topic_polarity = df_1.groupby('Topic')['Analysis_Polarity'].value_counts().unstack(fill_value=0).apply(lambda x: x / x.sum() * 100, axis=1)\n",
        "\n",
        "# Create the stacked bar plot\n",
        "ax = df_topic_polarity.plot(kind='bar',color=sns.palettes.mpl_palette('flare'), stacked=True, figsize=(15, 10))\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Topic')\n",
        "ax.set_ylabel('% Polarity')\n",
        "ax.set_title('Topic Polarity Distribution')\n",
        "\n",
        "# Add topic names to the x-axis\n",
        "ax.set_xticklabels(df_topic_polarity.index, rotation=90)\n",
        "\n",
        "# Add legend\n",
        "ax.legend(title='Polarity')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uJpARbYZwAq"
      },
      "source": [
        "###Perform Topic Labeling to analyze the polarity score of each of the topics identified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USxu-75X6RgX"
      },
      "outputs": [],
      "source": [
        "# Define the data\n",
        "topic_names = [\n",
        "    \"Insider Trading\",\n",
        "    \"AI & Data Centers\",\n",
        "    \"Chinese Market\",\n",
        "    \"Company Acquisitions\",\n",
        "    \"Defense Collaborations\",\n",
        "    \"Energy&Power Sector\",\n",
        "    \"Financial Reports\",\n",
        "    \"Stock Performance\",\n",
        "    \"Price Targets\",\n",
        "    \"Stock Disclosures\",]\n",
        "\n",
        "# Get the topic-polarity matrix\n",
        "topic_polarity_matrix = df_topic_polarity.values\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = np.corrcoef(topic_polarity_matrix)\n",
        "fig, ax = plt.subplots(figsize=(18, 13))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"flare\", fmt=\".1f\", xticklabels=topic_names, yticklabels=topic_names)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3WwKMCpZAjL"
      },
      "source": [
        "###Plot dendongram chart for the topic identified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6H9_1Ul7qEW"
      },
      "outputs": [],
      "source": [
        "\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the topic-polarity matrix\n",
        "scaler = StandardScaler()\n",
        "scaled_topic_polarity_matrix = scaler.fit_transform(topic_polarity_matrix)\n",
        "\n",
        "# Calculate the linkage\n",
        "linkage_matrix = sch.linkage(scaled_topic_polarity_matrix, method='ward')\n",
        "\n",
        "# Create the dendrogram\n",
        "plt.figure(figsize=(30, 20))\n",
        "sch.dendrogram(linkage_matrix, orientation='top', distance_sort='descending', labels=topic_names)\n",
        "plt.title('Dendrogram of Topic Polarity', size=24)\n",
        "plt.xlabel('Topics', size=20)\n",
        "plt.ylabel('Distance', size=20)\n",
        "plt.tick_params(axis='x', rotation=90)\n",
        "plt.tick_params(axis='y', which='major', labelsize=15)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5qSP6OU8uyj"
      },
      "outputs": [],
      "source": [
        "# Create a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes to the graph\n",
        "G.add_nodes_from(topic_names)\n",
        "\n",
        "# Add edges to the graph based on the polarity matrix\n",
        "for i in range(len(topic_polarity_matrix)):\n",
        "    for j in range(len(topic_polarity_matrix[0])):\n",
        "        if topic_polarity_matrix[i][j] > 0.5:\n",
        "            G.add_edge(topic_names[i], topic_names[j], weight=topic_polarity_matrix[i][j])\n",
        "\n",
        "# Set the layout of the nodes\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "# Draw the graph\n",
        "nx.draw(G, pos, with_labels=True, font_weight='bold')\n",
        "\n",
        "# Set the edge labels\n",
        "edge_labels = {(u, v): f'{d[\"weight\"]:.2f}' for u, v, d in G.edges(data=True)}\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0kyyz7qZTUj"
      },
      "source": [
        "###Load libraries for Network Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Yu23OVZdD_"
      },
      "source": [
        "###Check for the degree of centrality and betweenness for the topics identified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLUZLeTs992l"
      },
      "outputs": [],
      "source": [
        "# Calculate degree centrality\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "\n",
        "# Calculate betweenness centrality\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "# Print the results\n",
        "print(\"Degree Centrality:\")\n",
        "for node, centrality in degree_centrality.items():\n",
        "    print(f\"{node}: {centrality}\")\n",
        "\n",
        "print(\"\\nBetweenness Centrality:\")\n",
        "for node, centrality in betweenness_centrality.items():\n",
        "    print(f\"{node}: {centrality}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiMBouHxrb5D"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Assuming 'Analysis_Polarity' is your target variable and other relevant features are in X\n",
        "X = df[['Polarity', 'Subjectivity', 'Topic']]  # Example features, replace with your actual features\n",
        "y = df['Analysis_Polarity']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define models\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(solver='lbfgs', multi_class='auto')))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVC', SVC(gamma='scale')))\n",
        "models.append(('RFC', RandomForestClassifier(n_estimators=100)))\n",
        "models.append(('DTC', DecisionTreeClassifier()))\n",
        "models.append(('GBC', GradientBoostingClassifier()))\n",
        "\n",
        "\n",
        "# Evaluate models\n",
        "results = []\n",
        "names = []\n",
        "for name, model in models:\n",
        "    try:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Model: {name}, Accuracy: {accuracy}\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        results.append(accuracy)\n",
        "        names.append(name)\n",
        "    except Exception as e:  # Handle potential errors during model training/prediction\n",
        "        print(f\"Error with model {name}: {e}\")\n",
        "        results.append(0)\n",
        "        names.append(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiqTzmh-dUl5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi2gRdDKViof"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"flare\",\n",
        "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "            yticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "            annot_kws={\"size\": 13})\n",
        "plt.title(\"Confusion Matrix DTC\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
