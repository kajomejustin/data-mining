---
title: 'Chapter 4 Exercises: Classification'
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Solutions to selected exercises from Chapter 4 of *An Introduction to Statistical Learning with Applications in R*. Focuses on classification concepts like KNN, odds, and LDA.

## Exercise 4: The Curse of Dimensionality

This exercise explores why KNN struggles with high-dimensional data (Section 4.5.2, p. 164).

### (a) p = 1

For one feature uniformly distributed on [0, 1], we use observations within 10% of the range (0.1). The fraction of observations is:

$$ \text{Fraction} = \frac{0.1}{1} = 0.1 $$

**Answer**: 10% of observations.

### (b) p = 2

For two features in [0, 1] × [0, 1], the fraction is:

$$ \text{Fraction} = 0.1 \times 0.1 = 0.01 $$

**Answer**: 1% of observations.

### (c) p = 100

For 100 features, the fraction is:

$$ \text{Fraction} = 0.1^{100} = 10^{-100} $$

**Answer**: A negligible fraction, $10^{-100}$.

### (d) Drawback of KNN with Large p

As $p$ increases, the fraction of observations in the neighborhood ($0.1^p$) becomes tiny, making predictions unreliable due to sparse data (curse of dimensionality, p. 115, 193, 266).

**Answer**: KNN fails because very few observations are near the test point, leading to poor predictions.

### (e) Hypercube Side Length for 10% of Observations

To include 10% of observations, the hypercube volume is 0.1:

$$ s^p = 0.1 $$
$$ s = 0.1^{1/p} $$

Compute for different $p$:

```{r}
p_values <- c(1, 2, 100)
for (p in p_values) {
  s <- 0.1^(1/p)
  cat(sprintf("p = %d: Side length = %.3f\n", p, s))
}
```

**Answer**:
- $p = 1$: Side length = 0.1
- $p = 2$: Side length ≈ 0.316
- $p = 100$: Side length ≈ 0.977

As $p$ increases, the neighborhood spans nearly the entire space, losing locality.

## Exercise 9: Odds

Explores odds in the context of logistic regression (Section 4.3, p. 138–145).

### (a) Fraction Defaulting with Odds of 0.37

$$ \text{Odds} = \frac{P}{1 - P} = 0.37 $$

Solve for $P$:

```{r}
odds <- 0.37
P <- odds / (1 + odds)
cat(sprintf("Fraction defaulting: %.3f or %.1f%%\n", P, P*100))
```

**Answer**: 27% default.

### (b) Odds for 16% Default Probability

$$ P = 0.16 $$
$$ \text{Odds} = \frac{P}{1 - P} $$

```{r}
P <- 0.16
odds <- P / (1 - P)
cat(sprintf("Odds of default: %.4f\n", odds))
```

**Answer**: Odds = 0.1905.

## Exercise 14: Predicting Gas Mileage with Auto Dataset

Uses LDA to classify cars based on gas mileage (Section 4.4, p. 146–155).

### (a) Create Binary Variable mpg01

Create `mpg01`: 1 if `mpg` > median, 0 otherwise.

```{r}
library(ISLR2)
data(Auto)

median_mpg <- median(Auto$mpg)
Auto$mpg01 <- ifelse(Auto$mpg > median_mpg, 1, 0)
head(Auto[, c("mpg", "mpg01")])
```

**Answer**: `mpg01` created and added to the dataset.

### (b) Graphical Exploration

Explore associations with `mpg01` using boxplots and scatterplots.

```{r}
library(ggplot2)
library(gridExtra)

# Boxplots for quantitative variables
quant_vars <- c("cylinders", "displacement", "horsepower", "weight", "acceleration", "year")

plot_list <- list()
for (i in seq_along(quant_vars)) {
  var <- quant_vars[i]
  p <- ggplot(Auto, aes(x = factor(mpg01), y = .data[[var]])) +
    geom_boxplot() +
    labs(x = "mpg01", y = var, title = paste(var, "vs mpg01")) +
    theme_minimal()
  plot_list[[i]] <- p
}

# Arrange plots in a grid
grid.arrange(grobs = plot_list, ncol = 3)
```

```{r}
# Scatterplot matrix
library(GGally)
vars_to_plot <- c("displacement", "horsepower", "weight", "acceleration", "mpg01")
ggpairs(Auto[, vars_to_plot], aes(color = factor(mpg01)), 
        title = "Scatterplot Matrix")
```

**Findings**: `displacement`, `horsepower`, `weight`, and `cylinders` show strong associations with `mpg01`. Lower values correspond to `mpg01 = 1` (high mileage).

### (c) Split Data

Split into 70% training, 30% test.

```{r}
set.seed(42)
n <- nrow(Auto)
train_indices <- sample(1:n, size = floor(0.7 * n))

# Select predictors based on part (b) findings
predictors <- c("displacement", "horsepower", "weight", "cylinders")
X_train <- Auto[train_indices, predictors]
X_test <- Auto[-train_indices, predictors]
y_train <- Auto$mpg01[train_indices]
y_test <- Auto$mpg01[-train_indices]

cat(sprintf("Training set size: %d, Test set size: %d\n", 
            length(train_indices), length(y_test)))
```

**Answer**: Data split into training and test sets.

### (d) Perform LDA and Compute Test Error

Use LDA to predict `mpg01` and compute test error.

```{r}
library(MASS)

# Prepare training data
train_data <- Auto[train_indices, c(predictors, "mpg01")]

# Fit LDA model
lda_model <- lda(mpg01 ~ displacement + horsepower + weight + cylinders, 
                 data = train_data)

# Make predictions on test set
test_data <- Auto[-train_indices, predictors]
lda_pred <- predict(lda_model, newdata = test_data)
predictions <- lda_pred$class

# Calculate test error
test_error <- mean(predictions != y_test)
accuracy <- mean(predictions == y_test)

cat(sprintf("Test error: %.4f\n", test_error))
cat(sprintf("Test accuracy: %.4f\n", accuracy))
```

**Answer**: Test error is approximately 0.12 (exact value depends on the split), indicating good performance.